{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First part of laboratories were conducted by <a href=\"https://github.com/makskliczkowski\">Maksymilian Kliczkowski</a>. He created notebooks we used in laboratories and he help me and my friends with understanding basic and advanced topics in machine learning. I am really glad that I had him as a tutor.\n",
    "\n",
    "In first course of ML/AI we were build ML models from the ground using math.\n",
    "\n",
    "In this notebook I will present only some task we had as a homework from notebook about scikit-learn"
   ],
   "id": "9045f98717984b1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Viznet",
   "id": "4ae30bab54e7f04e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install viznet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import viznet\n",
    "from viznet import NodeBrush, EdgeBrush\n",
    "\n",
    "def _show():\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Task 0.5 Use `vis-net` to create an image of network.",
   "id": "2855fcb2834ca160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "oNodesN = 12 + 3\n",
    "inNodesN= 12 + 3\n",
    "grid    = viznet.Grid((7.0, 2.0), offset=(1, 1))\n",
    "brush   = viznet.NodeBrush('nn.input')\n",
    "hbrush1 = viznet.NodeBrush('nn.hidden')\n",
    "obrush  = viznet.NodeBrush('nn.output')\n",
    "brushdot= viznet.NodeBrush('basic', size='tiny')\n",
    "colors  = iter(('r', 'b', 'g', 'yellow', 'cyan',\n",
    "                'r', 'b', 'g', 'yellow', 'cyan',\n",
    "                'r', 'b', 'g', 'yellow', 'cyan'))\n",
    "colors2  = iter(('r', 'b', 'g', 'yellow', 'cyan',\n",
    "                'r', 'b', 'g', 'yellow', 'cyan',\n",
    "                'r', 'b', 'g', 'yellow', 'cyan'))\n",
    "# create nodes\n",
    "inNodes = [(brush >> grid[0, i]) if not (i >= (inNodesN-3)//2 and i < (inNodesN-3)//2 + 3) else (brushdot >> grid[0, i]) for i in range(inNodesN)]\n",
    "hiddenNodes1 = [(hbrush1 >> grid[3, i]) if not (i >= (inNodesN-3)//2 and i < (inNodesN-3)//2 + 3) else (brushdot >> grid[3, i]) for i in range(inNodesN)]\n",
    "percNode= [(obrush >> grid[6, i]) if not (i >= (oNodesN-3)//2 and i < (oNodesN-3)//2 + 3) else (brushdot >> grid[6, i]) for i in range(oNodesN)]\n",
    "#[percNode[i].text(f'Out[{i}]', position='right') for i in range(oNodesN)]\n",
    "[hiddenNodes1[-1].text(f'W_H ReLu 264 nodes', position='top')]\n",
    "# create edges\n",
    "for i in range(inNodesN):\n",
    "    edge= viznet.EdgeBrush('-', lw=2., color=next(colors))\n",
    "    if i < (inNodesN-3)//2:\n",
    "        inNodes[i].text(i, position = 'left')\n",
    "        for j in range(inNodesN):\n",
    "          if j < (inNodesN-3)//2:\n",
    "            e   = edge >> (inNodes[i], hiddenNodes1[j])\n",
    "          elif j >= (inNodesN-3)//2 + 3:\n",
    "            e   = edge >> (inNodes[i], hiddenNodes1[j])\n",
    "    elif i >= (inNodesN-3)//2 + 3:\n",
    "        val = 132 - inNodesN + i\n",
    "        inNodes[i].text(val, position = 'left')\n",
    "        for j in range(inNodesN):\n",
    "            if j < (inNodesN-3)//2:\n",
    "              e   = edge >> (inNodes[i], hiddenNodes1[j])\n",
    "            elif j >= (inNodesN-3)//2 + 3:\n",
    "              e   = edge >> (inNodes[i], hiddenNodes1[j])\n",
    "    if i < (inNodesN-3)//2:\n",
    "        inNodes[i].text(i, position = 'left')\n",
    "        for j in range((oNodesN-3)//2):\n",
    "            e   = edge >> (hiddenNodes1[i], percNode[j])\n",
    "    elif i >= (inNodesN-3)//2 + 3:\n",
    "        val = 132 - inNodesN + i\n",
    "        inNodes[i].text(val, position = 'left')\n",
    "        for j in range((oNodesN-3)//2):\n",
    "            e   = edge >> (hiddenNodes1[i], percNode[j])\n",
    "\n",
    "for i in range(oNodesN):\n",
    "    edge= viznet.EdgeBrush('-', lw=2., color=next(colors2))\n",
    "    if i < (oNodesN-3)//2:\n",
    "        percNode[i].text(i, position = 'right')\n",
    "        for j in range(oNodesN):\n",
    "          if j < (oNodesN-3)//2:\n",
    "            e   = edge >> (hiddenNodes1[j], percNode[i])\n",
    "          elif j >= (oNodesN-3)//2 + 3:\n",
    "            e   = edge >> (hiddenNodes1[j], percNode[i])\n",
    "    elif i >= (oNodesN-3)//2 + 3:\n",
    "        val = 132 - oNodesN + i\n",
    "        percNode[i].text(val, position = 'right')\n",
    "        for j in range(oNodesN):\n",
    "            if j < (oNodesN-3)//2:\n",
    "              e   = edge >> (hiddenNodes1[j], percNode[i])\n",
    "            elif j >= (oNodesN-3)//2 + 3:\n",
    "              e   = edge >> (hiddenNodes1[j], percNode[i])\n",
    "\n",
    "_show()"
   ],
   "id": "28b8be28876dc77f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Task 1 - Google ReLU. Define the function. Plot it.",
   "id": "5fcf6a9c2fe4c54d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def myrelu(x : np.ndarray):\n",
    "    return np.where(x>0, x ,0)\n",
    "\n",
    "x = np.arange(-10, 10, 1e-2)\n",
    "y = myrelu(x)\n",
    "plt.plot(x, y, label = 'I am ReLU')\n",
    "plt.legend()"
   ],
   "id": "d26a22139ce4a104"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 2 - You know what to do, Softmax it is!\n",
    "- Determine the sizes of the matrices of the OL.\n",
    "- What is the derivative of Softmax function? Plot it on the same plot."
   ],
   "id": "2c117cb036f22871"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def mysoftmax(x : np.ndarray):\n",
    "    return np.e**x/(np.sum(np.e**x))\n",
    "\n",
    "def diffmysoftmax(x : np.ndarray):\n",
    "    SumE = np.sum(np.e**x)\n",
    "    sqSumE = np.power(SumE,2)\n",
    "    y = []\n",
    "    for x_i in x:\n",
    "        e_x = np.e**x_i\n",
    "        mul = e_x * (SumE - e_x)\n",
    "        y.append(mul/sqSumE)\n",
    "    return y\n",
    "\n",
    "\n",
    "x = np.arange(-10, 10, 1e-2)\n",
    "y = tf.keras.activations.softmax(tf.convert_to_tensor([x]))\n",
    "plt.plot(x, y[0], label = 'I am Softmax')\n",
    "plt.legend()\n",
    "y = mysoftmax(x)\n",
    "plt.plot(x, y, label = 'My Softmax')\n",
    "plt.legend()\n",
    "y = diffmysoftmax(x)\n",
    "plt.plot(x, y, label = 'Diff softmax')\n",
    "plt.legend()"
   ],
   "id": "fe4edc99911ab831"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 3 - reading and plotting the images\n",
    "- read the MNIST dataset from tensorflow\n",
    "- load the data into train and test samples\n",
    "- convert trainY and testY to OneHot encoding - for each sample in M, create a vector of size 10 with zeros everywhere except for one in the position corresponding to correct number\n",
    "- flatten the data to construct vectors, not matrices (28, 28, M) -> (784, M)\n",
    "- normalize images to [0, 1)\n",
    "- plot some numbers as images"
   ],
   "id": "8bed0c16da39c882"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# read the data to a Pandas dataframe from Kaggle:\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "#!implement me\n",
    "def toOneHot(Y):\n",
    "    new_Y = []\n",
    "    for y in Y:\n",
    "      y_i = np.zeros(10)\n",
    "      y_i[y] = 1\n",
    "      new_Y.append(y_i)\n",
    "    return np.array(new_Y)\n",
    "\n",
    "# split it to train and test\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "# check the shape of the newly created data\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "\n",
    "# plot some examples\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(trainX[i], label = f'{np.argmax(trainY[i])}')\n",
    "\n",
    "N = 5000\n",
    "# flatten it!\n",
    "trainX  = np.reshape(trainX, (trainX.shape[0], trainX.shape[1] * trainX.shape[2]))[:N] / 255.\n",
    "trainY  = toOneHot(trainY)[:N]\n",
    "testX   = np.reshape(testX, (testX.shape[0], testX.shape[1] * testX.shape[2])) / 255.\n",
    "testY   = toOneHot(testY)\n",
    "\n",
    "# check the shape of the newly created data again\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "_show()"
   ],
   "id": "2b2bec6281c7d45b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 4. Create a NN class! Finally!\n",
    "Fill the functions of the class template in order to create a full NN. How exciting!\n",
    "- create activation functions\n",
    "- create derivatives of the activation functions and the activation functions\n",
    "- create initialization of the weights\n",
    "- create the forward and backpropagation\n",
    "- create a method that updates the weights\n",
    "- create a gradient descent algorithm"
   ],
   "id": "13a6bee0f76ac585"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "Quick general class...\n",
    "'''\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Activation:\n",
    "\n",
    "    def apply(self, x):\n",
    "        pass\n",
    "\n",
    "    def derivative(self, x):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.derivative(x)\n",
    "\n",
    "    def __call__(self, x) -> Any:\n",
    "        return self.apply(x)\n",
    "\n",
    "############################\n",
    "\n",
    "class MyRelu(Activation):\n",
    "\n",
    "    def apply(self, x):\n",
    "        return np.where(x>0, x ,0)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0, 1, 0)\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.derivative(x)\n",
    "\n",
    "    def __call__(self, x) -> Any:\n",
    "        return MyRelu.apply(x)\n",
    "\n",
    "############################\n",
    "\n",
    "class MySoftMax(Activation):\n",
    "\n",
    "    def apply(self, x):\n",
    "        \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\n",
    "        x = x - np.amax(x, axis=0)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0) #sum to change\n",
    "\n",
    "    def derivative(self, x):\n",
    "        SumE = np.sum(np.e**x)\n",
    "        sqSumE = np.power(SumE,2)\n",
    "        y = []\n",
    "        for x_i in x:\n",
    "            e_x = np.e**x_i\n",
    "            mul = e_x * (SumE - e_x)\n",
    "            y.append(mul/sqSumE)\n",
    "        return y\n",
    "\n",
    "class MySigmoid(Activation):\n",
    "    def apply(self, x):\n",
    "        return 1/(1 + np.e**(-x))\n",
    "    def derivative(self, x):\n",
    "        return (1 - self.apply(x))*self.apply(x)"
   ],
   "id": "b7161580a51505f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MyNN:\n",
    "    def __init__(self,\n",
    "                inputShape,\n",
    "                outputShape,\n",
    "                hiddenLayers    : list[int],\n",
    "                activations     : list[Activation],\n",
    "                lr              = 1e-3,\n",
    "                verbose         = True\n",
    "                ):\n",
    "        '''\n",
    "        - inputShape    - shape of the input variables (M - number of training samples, N - number of input neurons, )\n",
    "        - outputShape   - shape of the output variables (10? :))\n",
    "        - hiddenLayers  - sizes of corresponding hidden layers\n",
    "        - activations   - activation functions of the hidden layers (list of Activation classes)\n",
    "        - lr            - learning rate for the NN\n",
    "        - verbose       - talk?\n",
    "        '''\n",
    "\n",
    "        # initialize the shape of the input variables\n",
    "\n",
    "        self.inputShape     = inputShape\n",
    "        self.outputShape    = outputShape\n",
    "        # initialize the shape of hidden layers\n",
    "        self.hiddenLayers   = hiddenLayers\n",
    "        self.nHidden        = len(hiddenLayers)\n",
    "        # the weights will be saved onto a list 'cause they have different dimensions\n",
    "        self.W              = []\n",
    "        self.b              = []\n",
    "        # save the activation functions\n",
    "        self.activations    = activations\n",
    "        # other\n",
    "        self.lr             = lr\n",
    "        self.verbose        = verbose\n",
    "\n",
    "    def initializeWeights(self):\n",
    "        '''\n",
    "        A function that initializes the parameters of the NN\n",
    "        '''\n",
    "        # start off with taking the shape of the input layer\n",
    "        previous_layer_size = self.inputShape[0]\n",
    "\n",
    "        # hidden layers weights\n",
    "        for i in range(self.nHidden):\n",
    "            W = (np.random.random((self.hiddenLayers[i],\n",
    "                                  previous_layer_size)) - 0.5) / 1e3  # number of neurons in HL x number of inputs of each neuron\n",
    "            b = (np.random.random((self.hiddenLayers[i], 1)) - 0.5) / 1e3\n",
    "            self.W.append(W)\n",
    "            self.b.append(b)\n",
    "            previous_layer_size = self.hiddenLayers[i]\n",
    "\n",
    "        # weights for output layer\n",
    "        W = (np.random.random((self.outputShape[0],\n",
    "                              previous_layer_size))  - 0.5) / 1e3 # number of neurons in HL x number of inputs of each neuron\n",
    "        b = (np.random.random((self.outputShape[0], 1)) - 0.5) / 1e3\n",
    "        self.W.append(W)\n",
    "        self.b.append(b)\n",
    "\n",
    "        if self.verbose:\n",
    "            for i, a in enumerate(self.W):\n",
    "                print(f\"\\t->W_{i}\", a.shape)\n",
    "                print(f\"\\t->b_{i}\", self.b[i].shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Define the forward propagation of the NN\n",
    "        - X - input to the first layer\n",
    "        '''\n",
    "        # append the values to the list to save them\n",
    "        if self.verbose:\n",
    "            print(\"\\t\\t\\tX:\", X.shape)\n",
    "        if X.shape[1] == self.inputShape[0] and X.shape[0] != self.inputShape[0]:\n",
    "            Z = [X.reshape((X.shape[1], X.shape[0]))]  # before activation, reshape for proper data shape\n",
    "            A = [X.reshape((X.shape[1], X.shape[0]))]  # after activation\n",
    "        elif X.shape[0] == self.inputShape[0]:\n",
    "            Z = [X]\n",
    "            A = [X]\n",
    "        else:\n",
    "            print(\n",
    "                \"Shape of the input is not correct. Number of input neurons is different than length of input vector.\")\n",
    "            return 1\n",
    "        \n",
    "        for i in range(self.nHidden + 1):  # + 1 because of output layer\n",
    "            Zi = np.dot(self.W[i], A[i]) + self.b[i]\n",
    "            Ai = self.activations[i].apply(Zi)\n",
    "            A.append(Ai)\n",
    "            Z.append(Zi)\n",
    "        if self.verbose:\n",
    "            for i, a in enumerate(A):\n",
    "                print(f\"\\t\\t->A_{i}\", a.shape)\n",
    "                print(f\"\\t\\t->Z_{i}\", Z[i].shape)\n",
    "                \n",
    "        return A, Z  # the first index will be the output of NN\n",
    "\n",
    "    def backward(self, Y, A, Z):\n",
    "        '''\n",
    "        Define the backpropagation of the NN\n",
    "        - output labels!\n",
    "        - A - outputs of the forward propagation - activated\n",
    "        - Z - outputs of the forward propagation - not activated\n",
    "        '''\n",
    "        # append the values to the list to save them\n",
    "        if Y.shape[1] == self.outputShape[0] and Y.shape[0] == A[-1].shape[1]:\n",
    "            Y = Y.reshape((Y.shape[1], Y.shape[0]))  # reshape for proper data shape\n",
    "        elif Y.shape[0] != self.outputShape[0] or Y.shape[1] != A[-1].shape[1]:\n",
    "            print(\n",
    "                \"Shape of the input is not correct. Number of input neurons is different than length of input vector.\")\n",
    "            return 1\n",
    "        \n",
    "        dW = []\n",
    "        db = []\n",
    "        \n",
    "        M       = Y.shape[0]\n",
    "        dZi     = Y\n",
    "        deriv   = Y\n",
    "        for i in range(self.nHidden + 1):\n",
    "            k = self.nHidden - i + 1\n",
    "            if i == 0:\n",
    "                dZi     = A[self.nHidden + 1] - Y\n",
    "            else:\n",
    "                deriv   = self.activations[self.nHidden - i].derivative(Z[k])\n",
    "                # start from the left side by the previous weights\n",
    "                dZi     = dZi.dot(self.W[k])\n",
    "                # elementwise\n",
    "                dZi     = dZi * deriv\n",
    "        \n",
    "                dWi = 1.0 / M * dZi.T.dot(A[self.nHidden - i])\n",
    "                dbi = np.mean(dZi, axis = 0)[:, None] \n",
    "                dW.append(dWi)\n",
    "                db.append(dbi)\n",
    "        if self.verbose:\n",
    "            for i in range(self.nHidden + 1):\n",
    "                print(f\"\\t\\t->dW_{i}\", dW[i].shape)\n",
    "                print(f\"\\t\\t->db_{i}\", db[i].shape)\n",
    "        return dW[::-1], db[::-1]\n",
    "\n",
    "    def update(self, dW, db):\n",
    "        '''\n",
    "        Having the gradients - update the weights!\n",
    "        '''\n",
    "\n",
    "        for i in range(self.nHidden + 1):\n",
    "          self.W[i] = self.W[i] - self.lr * dW[i]\n",
    "          self.b[i] = self.b[i] - self.lr * db[i]\n",
    "\n",
    "\n",
    "    def getAccuracy(yTrue, yPred):\n",
    "        '''\n",
    "        Check the accuracy of the prediction\n",
    "        - yTrue - true labels put to the NN\n",
    "        - yPred - predicted labels\n",
    "        '''\n",
    "        # return yTrue == yPred\n",
    "        return np.argmax(yTrue, axis = 0) == np.argmax(yPred, axis = 0)\n",
    "\n",
    "    def gradientDescent(self, X, Y, iterations):\n",
    "        '''\n",
    "        Loop to update the NN parameters (with gradient descent)\n",
    "        - X          - images\n",
    "        - Y          - labels\n",
    "        - iterations - number of interations to finish\n",
    "        '''\n",
    "        # initialize the weights\n",
    "        self.initializeWeights()\n",
    "        print(\"Y:\", Y.shape)\n",
    "        history = []\n",
    "        # iterate\n",
    "        for i in range(iterations):\n",
    "            # calculate forward propagation\n",
    "            A, Z    = self.forward(X)\n",
    "            # calculate backward propagation\n",
    "            dW, db  = self.backward(Y, A, Z)\n",
    "            # update the weights\n",
    "            self.update(dW, db)\n",
    "\n",
    "            # get the accuracy\n",
    "            accuracy = MyNN.getAccuracy(Y, A[0])\n",
    "            accuracy = np.mean(accuracy)\n",
    "            history.append(np.mean(accuracy))\n",
    "\n",
    "            # print accuracy\n",
    "            if i % 50 == 0:\n",
    "                print(np.array(A[0])[:,0])\n",
    "                print(list(Y[:,0]))\n",
    "                print(f\"Iteration [{i}/{iterations}]\")\n",
    "                print(f\"Accuracy: {accuracy:.3f}\")\n",
    "                print('-------------------------')\n",
    "        return history\n",
    "\n",
    "    def train(self, X, Y, iterations):\n",
    "        return self.gradientDescent(X, Y, iterations)\n",
    "\n",
    "    def test(self, X, Y):\n",
    "        A, Z = self.forward(X)\n",
    "        acc  = MyNN.getAccuracy(Y, A[-1])\n",
    "        return acc, np.mean(acc), A[-1]"
   ],
   "id": "608b5bc0ab55744d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "myNN = MyNN(\n",
    "    inputShape      = (784,),\n",
    "    outputShape     = (10,),\n",
    "    hiddenLayers    = [120],\n",
    "    activations     = [MyRelu(), MySoftMax()],\n",
    "    lr              = 5e-1,\n",
    "    verbose         = False\n",
    "    )\n",
    "history = myNN.gradientDescent(\n",
    "    X               = trainX, \n",
    "    Y               = trainY,\n",
    "    iterations      = 500\n",
    "    )\n",
    "score, mean, predictions = myNN.test(testX, testY)"
   ],
   "id": "ea69c40b77dd6ec2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 4.5 Test the score!\n",
    "- plot 5 random samples and label corresponding predictions\n",
    "- plot the confusion matrix of the samples, use `scikit learn` <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html>"
   ],
   "id": "52d3dc0126947ec3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "FIGX    = 2\n",
    "FIGY    = 3\n",
    "fig, ax = plt.subplots(FIGX, FIGY)\n",
    "axes    = []\n",
    "for x in range(FIGX):\n",
    "    for y in range(FIGY):\n",
    "        axes.append(ax[x, y])\n",
    "for i, prediction in enumerate(predictions.T[:FIGX * FIGY]):\n",
    "    axes[i].imshow(testX[i].reshape(28, 28))\n",
    "    axes[i].set_title(f\"Prediction: {np.argmax(prediction)}\")\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = np.argmax(predictions, axis = 0)\n",
    "y_true = np.argmax(testY, axis = 1)\n",
    "# y_pred.shape, y_true.shape\n",
    "\n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(2)\n",
    "plt.imshow(confusion)"
   ],
   "id": "a85bbf15bf3e9d66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
