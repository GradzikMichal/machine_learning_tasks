{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Second part of laboratories were conducted by <a href=\"https://github.com/jarek-pawlowski\">Jarosław Pawłowski</a>. He created notebooks we used in laboratories. I am really glad that I had him as a tutor.\n",
    "\n",
    "In second course of ML/AI we were build ML models using pytorch.\n",
    "\n",
    "In this notebook I will present only some task we had as a homework from notebook."
   ],
   "id": "82e1803c5ada7811"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Regularization",
   "id": "b3146a0168adce8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributed._spmd.api import Override\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "model_args = {}\n",
    "# we will use batch size of 64 in Stochastic Gradient Descent (SGD) optimization of the network\n",
    "model_args['batch_size'] = 64\n",
    "# learning rate is how fast it will descend\n",
    "model_args['lr'] = .07\n",
    "# the number of epochs is the number of times you go through the full dataset\n",
    "model_args['epochs'] = 20\n",
    "# L2 (ridge) penalty\n",
    "model_args['L2_lambda'] = 5.e-3\n",
    "# L1 (LASSO) penalty\n",
    "model_args['L1_lambda'] = 2.e-4\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define problem\n",
    "- we are going to use CIFAR-10 [dataset](https://www.cs.toronto.edu/~kriz/cifar.html) of 32x32 color images to train image classifier,\n",
    "- in order to test various regularizations our vanilla CNN classifier should be overfitted."
   ],
   "id": "f5ce02558fe1c2a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download the dataset",
   "id": "adecaca1ae76a2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!rm -r ./data\n",
    "# normalize dataset\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean, std)])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# we divide this data into training and validation subsets\n",
    "train_subset, validation_subset = torch.utils.data.random_split(cifar10_train, [40000, 10000])\n",
    "test_subset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# subsample to speedup training (colab has notebook lifetime limit)\n",
    "train_subset = torch.utils.data.Subset(train_subset, range(20000))\n",
    "validation_subset = torch.utils.data.Subset(validation_subset, range(5000))\n",
    "test_subset = torch.utils.data.Subset(test_subset, range(5000))\n",
    "\n",
    "# define dataloaders\n",
    "loader_kwargs = {'batch_size': model_args['batch_size'],\n",
    "                 'num_workers': 2,\n",
    "                 'pin_memory': True,\n",
    "                 'shuffle': True}\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, **loader_kwargs)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_subset, **loader_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, **loader_kwargs)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "id": "933a4315ec5eea98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_subset = torch.load('train_subset.pt', map_location=torch.device('cuda:0'))\n",
    "\n",
    "validation_subset = torch.load('validation_subset.pt', map_location=torch.device('cuda:0'))\n",
    "test_cifar10 = torch.load('test_cifar10.pth', map_location=torch.device('cuda:0'))\n",
    "# subsample to speedup training (colab has notebook lifetime limit)\n",
    "train_subset = torch.utils.data.Subset(train_subset, range(20000))\n",
    "validation_subset = torch.utils.data.Subset(validation_subset, range(5000))\n",
    "test_subset = torch.utils.data.Subset(test_cifar10, range(5000))\n",
    "for x in train_subset:\n",
    "    print(x)\n",
    "    break"
   ],
   "id": "a9eec24a47c62ff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define dataloaders\n",
    "print(type(train_subset))\n",
    "# train_subset = torch.utils.data.TensorDataset(train_subset)\n",
    "\n",
    "train_loader = []\n",
    "train_data = []\n",
    "rowx = []\n",
    "rowy = []\n",
    "i = 0\n",
    "for (x, y) in train_subset:\n",
    "    if i % model_args['batch_size'] == model_args['batch_size']-1:\n",
    "        train_data.append([torch.stack(rowx, dim =0).to(\"cuda:0\"), torch.tensor(rowy, device=\"cuda:0\")])\n",
    "        rowx = []\n",
    "        rowy = []\n",
    "    else:\n",
    "        rowx.append(x)\n",
    "        rowy.append(y)\n",
    "    i += 1\n",
    "print(1)\n",
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validation_loader = []\n",
    "rowx = []\n",
    "rowy = []\n",
    "i = 0\n",
    "for x, y in validation_subset:\n",
    "    if i % model_args['batch_size'] == model_args['batch_size'] - 1:\n",
    "        validation_loader.append([torch.stack(rowx, dim =0).to(\"cuda:0\"), torch.tensor(rowy, device=\"cuda:0\")])\n",
    "        rowx = []\n",
    "        rowy = []\n",
    "    else:\n",
    "        rowx.append(x)\n",
    "        rowy.append(y)\n",
    "    i += 1\n",
    "print(1)\n",
    "print(len(validation_loader))\n",
    "print(validation_loader[0])\n",
    "\n",
    "\n",
    "\n",
    "test_loader = []\n",
    "rowx = []\n",
    "rowy = []\n",
    "i = 0\n",
    "for x, y in test_subset:\n",
    "    if i % model_args['batch_size'] == model_args['batch_size'] - 1:\n",
    "        \n",
    "        test_loader.append([torch.stack(rowx, dim =0).to(\"cuda:0\"), torch.tensor(rowy, device=\"cuda:0\")])\n",
    "        rowx = []\n",
    "        rowy = []\n",
    "    else:\n",
    "        rowx.append(x)\n",
    "        rowy.append(y)\n",
    "    i += 1\n",
    "print(1)\n",
    "print(len(test_loader))\n",
    "print(test_loader[0])\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "id": "20547843ba50e911"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import inspect\n",
    "class SimpleCNN(nn.Module):\n",
    "    '''\n",
    "    simple CNN model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model\n",
    "    '''\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "vgg_cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    'vgg22': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 256, 'M', 512, 512, 512, 512, 512, 'M', 512, 512, 512, 512, 512, 'M']\n",
    "}\n",
    "\n",
    "\n",
    "def train_with_regularization(model, device, train_data, optimizer, criterion,\n",
    "                              epoch_number,\n",
    "                              L1_lambda,\n",
    "                              L2_lambda):\n",
    "    model.train()\n",
    "    train_loss = torch.tensor(0., device=device)\n",
    "    # get subsequent batches over the data in a given epoch\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        # send data tensors to GPU (or CPU)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # this will zero out the gradients for this batch\n",
    "        optimizer.zero_grad()\n",
    "        #print(inspect.getmembers(optimizer, predicate=inspect.ismethod))\n",
    "        # this will execute the forward() function\n",
    "        output = model(data)\n",
    "        # calculate loss using c\n",
    "        loss = criterion(output, target)\n",
    "        # L2 regularization implemeted by hand\n",
    "        \n",
    "        L2_norm = sum((p**2).sum() for p in model.parameters())\n",
    "        # L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        #\n",
    "        loss_regularized = loss + L1_norm*L1_lambda + L2_norm*L2_lambda\n",
    "        \n",
    "        # backpropagate the loss\n",
    "        loss_regularized.backward()\n",
    "        # update the model weights (with assumed learning rate)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print('Train Epoch: {}'.format(epoch_number))\n",
    "    train_loss_len = torch.tensor(len(train_data), device=device)\n",
    "    train_loss /= train_loss_len\n",
    "    print('\\tTrain set: Average loss: {:.4f}'.format(train_loss))\n",
    "    return train_loss\n",
    "\n",
    "def test(model, device, test_data, criterion, message=None):\n",
    "    model.eval()\n",
    "    test_loss = torch.tensor(0., device=device)\n",
    "    correct = torch.tensor(0., device=device)\n",
    "    # this is just inference, we don't need to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_data:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # calculate and sum up batch loss\n",
    "            test_loss += criterion(output, target)\n",
    "            # get the index of class with the max probability\n",
    "            prediction = output.argmax(dim=1)\n",
    "            #_, predicted = torch.max(outputs.data, axis=1)\n",
    "            # item() returns value of the given tensor\n",
    "            correct += prediction.eq(target).sum().item()\n",
    "    test_loss /= len(test_data)\n",
    "    accuracy = correct / len(test_data)\n",
    "    if message is not None:\n",
    "        print('\\t{}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            message, test_loss, correct, len(test_data)*model_args['batch_size'], 100.*accuracy/model_args['batch_size']))\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def run_training(model, device, criterion, optimizer, no_epochs,\n",
    "                 L1_lambda=0.,\n",
    "                 L2_lambda=0.):\n",
    "    train_loss = torch.tensor([0.], device=device)\n",
    "    validation_loss = torch.tensor([0.], device=device)\n",
    "    validation_accuracy = torch.tensor([0.], device=device)\n",
    "    test_accuracy = torch.tensor([0.], device=device)\n",
    "    for epoch_number in range(1, no_epochs+1):\n",
    "        train_loss = torch.cat((train_loss, torch.tensor([train_with_regularization(model, device, train_data,\n",
    "                                optimizer, criterion, epoch_number,\n",
    "                                L1_lambda, L2_lambda)], device=device)))\n",
    "        \n",
    "        val_loss, val_acc = test(model, device, validation_loader,\n",
    "                                criterion, 'Validation set')\n",
    "        \n",
    "        validation_loss = torch.cat((validation_loss, torch.tensor([val_loss], device=device)))\n",
    "        validation_accuracy = torch.cat((validation_accuracy, torch.tensor([val_acc], device=device)))\n",
    "        # we also collect test accuracies for every epoch\n",
    "        _, test_acc = test(model, device, test_loader, criterion)\n",
    "        test_accuracy = torch.cat((test_accuracy, torch.tensor([test_acc], device=device)))\n",
    "    # and select test accuracy for the best epoch (with the highest validation accuracy)\n",
    "    train_loss = train_loss[1:]\n",
    "    validation_loss = validation_loss[1:]\n",
    "    validation_accuracy = validation_accuracy[1:]\n",
    "    test_accuracy = test_accuracy[1:]\n",
    "    best_accuracy = test_accuracy[torch.argmax(validation_accuracy)]\n",
    "    return train_loss, validation_loss, best_accuracy\n",
    "\n",
    "def plot_loss(train_loss, validation_loss, title):\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"subsequent epochs\")\n",
    "    plt.ylabel('average loss')\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, 'o-', label='training')\n",
    "    plt.plot(range(1, len(validation_loss)+1), validation_loss, 'o-', label='validation')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ],
   "id": "8c1db28ba552855"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tasks to do:\n",
    "\n",
    "0. Try to play with L2 and L1 *lambdas* do get highest possible accuracy.\n",
    "- e.g. what if we increase *L1_lambda* to be the same as *L2_lambda*\n",
    "1. Repeat the calculations but for the VGG11 model.\n",
    "2. Compare amplitudes (plot histograms?) of model weights for the L2 and L1 case -- is L1 a strong feature selector?\n",
    "\n",
    "3. Reimplement pytorch [SGD method](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD) to get L1 regularization (|w| instead of w**2)\n",
    "\n",
    "Hint: change line\n",
    "```python\n",
    "if weight_decay != 0:\n",
    "    d_p = d_p.add(param, alpha=weight_decay)\n",
    "```\n",
    "to\n",
    "```python\n",
    "if weight_decay != 0:\n",
    "    d_p = d_p.add(torch.sign(param), alpha=weight_decay)\n",
    "```"
   ],
   "id": "5bfc2c0a88a4dd31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0",
   "id": "3050de275f8a21cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = time.time()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=model_args['lr'],\n",
    "                      weight_decay=0.)\n",
    "\n",
    "train_loss_l1_l2, val_loss_l1_l2, best_accuracy = run_training(model, device, criterion, optimizer,\n",
    "                                                         torch.tensor(model_args['epochs']*4, device=device),\n",
    "                                                         L1_lambda=torch.tensor(0.0005, device=device),\n",
    "                                                         L2_lambda=torch.tensor(model_args['L2_lambda'], device=device))\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy/model_args[\"batch_size\"]))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time needed: {:.0f}s\".format(end - start))\n",
    "plot_loss(train_loss_l1_l2.cpu(), val_loss_l1_l2.cpu(), 'L1 and L2 implemented by hand')"
   ],
   "id": "282240e4bd1cf0b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1",
   "id": "23ac513e586c3da2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = time.time()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_l1 = VGG(make_layers(vgg_cfg['vgg11'])).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model_l1.parameters(),\n",
    "                      lr=model_args['lr'],\n",
    "                      weight_decay=0.)\n",
    "no_epochs = torch.tensor(model_args['epochs'], device=device)\n",
    "\n",
    "train_loss_11, val_loss_11, best_accuracy = run_training(model_l1, device, criterion, optimizer,\n",
    "                                                         torch.tensor(model_args['epochs'], device=device),\n",
    "                                                         L1_lambda=torch.tensor(0.0005, device=device),\n",
    "                                                         L2_lambda=torch.tensor(model_args['L2_lambda'], device=device))\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy/model_args['batch_size']))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time needed: {:.0f}s\".format(end - start))\n",
    "plot_loss(train_loss_11.cpu(), val_loss_11.cpu(), 'VGG11 model')"
   ],
   "id": "d99ad9b5d4d7cd1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.",
   "id": "48290189ca6ac64c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_l1.parameters()",
   "id": "a26088ebbd67419"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3.\n",
    "\n",
    "Trying to rewrite pytorch model"
   ],
   "id": "3eea7e9b50001f0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.optim.optimizer import _group_tensors_by_device_and_dtype\n",
    "from torch.optim.optimizer import _use_grad_for_differentiable, _default_to_fused_or_foreach\n",
    "from typing import List, Optional\n",
    "from torch import Tensor\n",
    "from overrides import override\n",
    "\n",
    "\n",
    "class mySDG(optim.SGD):\n",
    "    def __init__(self, model, lr, weight_decay):\n",
    "        super().__init__(model, lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "    @_use_grad_for_differentiable\n",
    "    @override(check_at_runtime=True)\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            momentum_buffer_list = []\n",
    "\n",
    "            has_sparse_grad = self._init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n",
    "\n",
    "            sgd(params_with_grad,\n",
    "                d_p_list,\n",
    "                momentum_buffer_list,\n",
    "                weight_decay=group['weight_decay'],\n",
    "                momentum=group['momentum'],\n",
    "                lr=group['lr'],\n",
    "                dampening=group['dampening'],\n",
    "                nesterov=group['nesterov'],\n",
    "                maximize=group['maximize'],\n",
    "                has_sparse_grad=has_sparse_grad,\n",
    "                foreach=group['foreach'],\n",
    "                fused=group['fused'],\n",
    "                grad_scale=getattr(self, \"grad_scale\", None),\n",
    "                found_inf=getattr(self, \"found_inf\", None))\n",
    "\n",
    "            # update momentum_buffers in state\n",
    "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "                state = self.state[p]\n",
    "                state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss\n",
    "\n",
    "def sgd(params: List[Tensor],\n",
    "        d_p_list: List[Tensor],\n",
    "        momentum_buffer_list: List[Optional[Tensor]],\n",
    "        # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
    "        # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
    "        has_sparse_grad: bool = None,\n",
    "        foreach: Optional[bool] = None,\n",
    "        fused: Optional[bool] = None,\n",
    "        grad_scale: Optional[Tensor] = None,\n",
    "        found_inf: Optional[Tensor] = None,\n",
    "        *,\n",
    "        weight_decay: float,\n",
    "        momentum: float,\n",
    "        lr: float,\n",
    "        dampening: float,\n",
    "        nesterov: bool,\n",
    "        maximize: bool):\n",
    "    r\"\"\"Functional API that performs SGD algorithm computation.\n",
    "\n",
    "    See :class:`~torch.optim.SGD` for details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Respect when the user inputs False/True for foreach or fused. We only want to change\n",
    "    # the default when neither have been user-specified. Note that we default to foreach\n",
    "    # and pass False to use_fused. This is not a mistake--we want to give the fused impl\n",
    "    # bake-in time before making it the default, even if it is typically faster.\n",
    "    if foreach is None and fused is None:\n",
    "        # why must we be explicit about an if statement for torch.jit.is_scripting here?\n",
    "        # because JIT can't handle Optionals nor fancy conditionals when scripting\n",
    "        if not torch.jit.is_scripting():\n",
    "            fused, foreach = _default_to_fused_or_foreach(params, differentiable=False, use_fused=False)\n",
    "        else:\n",
    "            foreach = False\n",
    "            fused = False\n",
    "    if foreach is None:\n",
    "        foreach = False\n",
    "    if fused is None:\n",
    "        fused = False\n",
    "\n",
    "    if foreach and torch.jit.is_scripting():\n",
    "        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n",
    "    if fused and torch.jit.is_scripting():\n",
    "        raise RuntimeError('torch.jit.script not supported with fused optimizers')\n",
    "\n",
    "    if foreach and not torch.jit.is_scripting():\n",
    "        func = _multi_tensor_sgd\n",
    "    elif fused and not torch.jit.is_scripting():\n",
    "        func = _fused_sgd\n",
    "    else:\n",
    "        func = _single_tensor_sgd\n",
    "\n",
    "    func(params,\n",
    "         d_p_list,\n",
    "         momentum_buffer_list,\n",
    "         weight_decay=weight_decay,\n",
    "         momentum=momentum,\n",
    "         lr=lr,\n",
    "         dampening=dampening,\n",
    "         nesterov=nesterov,\n",
    "         has_sparse_grad=has_sparse_grad,\n",
    "         maximize=maximize,\n",
    "         grad_scale=grad_scale,\n",
    "         found_inf=found_inf)    \n",
    "    \n",
    "def _single_tensor_sgd(params: List[Tensor],\n",
    "                   d_p_list: List[Tensor],\n",
    "                   momentum_buffer_list: List[Optional[Tensor]],\n",
    "                   grad_scale: Optional[Tensor],\n",
    "                   found_inf: Optional[Tensor],\n",
    "                   *,\n",
    "                   weight_decay: float,\n",
    "                   momentum: float,\n",
    "                   lr: float,\n",
    "                   dampening: float,\n",
    "                   nesterov: bool,\n",
    "                   maximize: bool,\n",
    "                   has_sparse_grad: bool):\n",
    "    assert grad_scale is None and found_inf is None\n",
    "    for i, param in enumerate(params):\n",
    "        d_p = d_p_list[i] if not maximize else -d_p_list[i]\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            d_p = d_p.add(torch.sign(param), alpha=weight_decay)\n",
    "\n",
    "        if momentum != 0:\n",
    "            buf = momentum_buffer_list[i]\n",
    "\n",
    "            if buf is None:\n",
    "                buf = torch.clone(d_p).detach()\n",
    "                momentum_buffer_list[i] = buf\n",
    "            else:\n",
    "                buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "\n",
    "            if nesterov:\n",
    "                d_p = d_p.add(buf, alpha=momentum)\n",
    "            else:\n",
    "                d_p = buf\n",
    "\n",
    "        param.add_(d_p, alpha=-lr)\n",
    "\n",
    "def _multi_tensor_sgd(params: List[Tensor],\n",
    "                  grads: List[Tensor],\n",
    "                  momentum_buffer_list: List[Optional[Tensor]],\n",
    "                  grad_scale: Optional[Tensor],\n",
    "                  found_inf: Optional[Tensor],\n",
    "                  *,\n",
    "                  weight_decay: float,\n",
    "                  momentum: float,\n",
    "                  lr: float,\n",
    "                  dampening: float,\n",
    "                  nesterov: bool,\n",
    "                  maximize: bool,\n",
    "                  has_sparse_grad: bool):\n",
    "    assert grad_scale is None and found_inf is None\n",
    "    \n",
    "    if len(params) == 0:\n",
    "        return\n",
    "    grouped_tensors = _group_tensors_by_device_and_dtype([params, grads, momentum_buffer_list], with_indices=True)\n",
    "    for ((device_params, device_grads, device_momentum_buffer_list), indices) in grouped_tensors.values():\n",
    "        device_has_sparse_grad = has_sparse_grad and any(grad.is_sparse for grad in device_grads)\n",
    "        if maximize:\n",
    "            device_grads = torch._foreach_neg(device_grads)\n",
    "        paramss = []\n",
    "        for param in device_grads:\n",
    "            paramss.append(torch.sign(param))\n",
    "        if weight_decay != 0:\n",
    "            # Re-use the intermediate memory (device_grads) already allocated for maximize\n",
    "            \n",
    "            if maximize:\n",
    "                torch._foreach_add_(paramss, device_params, alpha=weight_decay)\n",
    "            else:\n",
    "                device_grads = torch._foreach_add(paramss, device_params, alpha=weight_decay)\n",
    "        if momentum != 0:\n",
    "            bufs = []\n",
    "\n",
    "            all_states_with_momentum_buffer = True\n",
    "            for i in range(len(device_momentum_buffer_list)):\n",
    "                if device_momentum_buffer_list[i] is None:\n",
    "                    all_states_with_momentum_buffer = False\n",
    "                    break\n",
    "                else:\n",
    "                    bufs.append(device_momentum_buffer_list[i])\n",
    "\n",
    "            if all_states_with_momentum_buffer:\n",
    "                torch._foreach_mul_(bufs, momentum)\n",
    "                torch._foreach_add_(bufs, device_grads, alpha=1 - dampening)\n",
    "            else:\n",
    "                bufs = []\n",
    "                for i in range(len(device_momentum_buffer_list)):\n",
    "                    if device_momentum_buffer_list[i] is None:\n",
    "                        buf = device_momentum_buffer_list[i] = momentum_buffer_list[indices[i]] = \\\n",
    "                            torch.clone(device_grads[i]).detach()\n",
    "                    else:\n",
    "                        buf = device_momentum_buffer_list[i]\n",
    "                        buf.mul_(momentum).add_(device_grads[i], alpha=1 - dampening)\n",
    "\n",
    "                    bufs.append(buf)\n",
    "\n",
    "            if nesterov:\n",
    "                torch._foreach_add_(device_grads, bufs, alpha=momentum)\n",
    "            else:\n",
    "                device_grads = bufs\n",
    "\n",
    "        if not device_has_sparse_grad:\n",
    "            torch._foreach_add_(device_params, device_grads, alpha=-lr)\n",
    "        else:\n",
    "            # foreach APIs don't support sparse\n",
    "            for i in range(len(device_params)):\n",
    "                device_params[i].add_(device_grads[i], alpha=-lr)\n",
    "def _fused_sgd(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    momentum_buffer_list: List[Optional[Tensor]],\n",
    "    grad_scale: Optional[Tensor],\n",
    "    found_inf: Optional[Tensor],\n",
    "    *,\n",
    "    weight_decay: float,\n",
    "    momentum: float,\n",
    "    lr: float,\n",
    "    dampening: float,\n",
    "    nesterov: bool,\n",
    "    maximize: bool,\n",
    "    has_sparse_grad: bool,\n",
    ") -> None:\n",
    "    if not params:\n",
    "        return\n",
    "    if has_sparse_grad:\n",
    "        raise RuntimeError(\"`_fused_sgd` does not support sparse gradients\")\n",
    "    grad_scale_dict = {grad_scale.device: grad_scale} if grad_scale is not None else None\n",
    "    found_inf_dict = {found_inf.device: found_inf} if found_inf is not None else None\n",
    "\n",
    "    no_momentum_buffer = momentum == 0\n",
    "    is_first_step = all(t is None for t in momentum_buffer_list) and not no_momentum_buffer\n",
    "    if is_first_step:\n",
    "        for i, g in enumerate(grads):\n",
    "            momentum_buffer_list[i] = torch.empty_like(g)\n",
    "    grouped_tensors = _group_tensors_by_device_and_dtype(\n",
    "        [params, grads, momentum_buffer_list], with_indices=False)\n",
    "    for (device, dtype), ((device_params, device_grads, device_momentum_buffer_list), _) in grouped_tensors.items():\n",
    "        device_grad_scale, device_found_inf = None, None\n",
    "        if grad_scale is not None:\n",
    "            if device not in grad_scale_dict:\n",
    "                grad_scale_dict[device] = grad_scale.to(device)\n",
    "            device_grad_scale = grad_scale_dict[device]\n",
    "        if found_inf is not None:\n",
    "            if device not in found_inf_dict:\n",
    "                found_inf_dict[device] = found_inf.to(device)\n",
    "            device_found_inf = found_inf_dict[device]\n",
    "        torch._fused_sgd_(\n",
    "            device_params,\n",
    "            device_grads,\n",
    "            [] if no_momentum_buffer else device_momentum_buffer_list,\n",
    "            weight_decay=weight_decay,\n",
    "            momentum=momentum,\n",
    "            lr=lr,\n",
    "            dampening=dampening,\n",
    "            nesterov=nesterov,\n",
    "            maximize=maximize,\n",
    "            is_first_step=is_first_step,\n",
    "            grad_scale=device_grad_scale,\n",
    "            found_inf=device_found_inf,\n",
    "        )\n"
   ],
   "id": "c7dbeac19e3a0e2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = mySDG(model.parameters(),\n",
    "                      lr=model_args['lr']*0.005,\n",
    "                      weight_decay=0.1)\n",
    "\n",
    "train_loss_0, val_loss_0, best_accuracy = run_training(model, device, criterion, optimizer, torch.tensor(model_args['epochs']*2, device=device))\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy/model_args['batch_size']))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time needed: {:.0f}s\".format(end - start))\n",
    "plot_loss(train_loss_0.cpu(), val_loss_0.cpu(), 'Without any regularization')\n"
   ],
   "id": "76086e39bcd6b649"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
