{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Second part of laboratories were conducted by <a href=\"https://github.com/jarek-pawlowski\">Jarosław Pawłowski</a>. He created notebooks we used in laboratories. I am really glad that I had him as a tutor.\n",
    "\n",
    "In second course of ML/AI we were build ML models using pytorch.\n",
    "\n",
    "In this notebook I will present only some task we had as a homework from notebook."
   ],
   "id": "bf4644bb1b620487"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model_args = {}\n",
    "model_args['batch_size'] = 128\n",
    "\n",
    "# learning rate is how fast it will descend\n",
    "model_args['lr'] = 1.e-3\n",
    "# the number of epochs is the number of times you go through the full dataset\n",
    "model_args['epochs'] = 40\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define scaled-dot product\n",
    "def scaled_dot_product(q, k, v):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / np.sqrt(d_k)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ],
   "id": "d86a73c467d71c38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define multi-head attention block\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "            Assumptions we made:\n",
    "            d_k = d_v = head_dim\n",
    "            num_heads*head_dim = embed_dim\n",
    "            d_out = embed_dim\n",
    "            D = d_model = input_dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        qkv = self.qkv_proj(x) # [batch_size, seq_length, 3*embed_dim]\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)  # [batch_size, seq_length, num_heads, 3*head_dim]\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, seq_length, 3*head_dim]\n",
    "        q, k, v = qkv.chunk(3, dim=-1) # [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v)\n",
    "        values = values.permute(0, 2, 1, 3) # [batch_size, seq_length, num_heads, head_dim]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim) # heads concatenation\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ],
   "id": "3fbb262cb4af5454"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward):\n",
    "        \"\"\"\n",
    "            input_dim - dimensionality of the input\n",
    "            num_heads - number of heads to use in the attention block\n",
    "            dim_feedforward - dimensionality of the hidden layer in the MLP\n",
    "            For simplicity we assume input_dim = embed_dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.MLP_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = x + attn_out  # residual connection\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.MLP_net(x)\n",
    "        x = x + linear_out  # residual connection\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps"
   ],
   "id": "dd4d0459f9103d30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "            d_model - input dim\n",
    "            max_len - maximum length of a sequence to expect\n",
    "            Encoding same as in the original paper\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ],
   "id": "f230d4aaa011d2d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ],
   "id": "6c322eab80e73a3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, num_layers):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim = model_dim - dimensionality to use inside the Transformer\n",
    "            num_heads - number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers - number of encoder blocks to use\n",
    "            lr - learning rate in the optimizer\n",
    "            warmup - number of warmup steps\n",
    "            max_iters - number of maximum iterations the model is trained for\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.input_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=self.num_layers,\n",
    "                                              input_dim=self.input_dim,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                              dim_feedforward=2*self.input_dim)\n",
    "\n",
    "    def forward(self, x, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "            x - input features of shape [Batch, SeqLen, input_dim]\n",
    "        \"\"\"\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x)\n",
    "        return attention_maps\n",
    "\n",
    "\n",
    "class TrainingUtils():\n",
    "\n",
    "    def __init__(self, device, model, num_classes, add_positional_encoding):\n",
    "        self.model = model.to(device)\n",
    "        self.num_classes = num_classes\n",
    "        self.positional_encoding = add_positional_encoding\n",
    "\n",
    "    def define_optimizer(self, lr, warmup, max_iters, use_lr_scheduler=True):\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        if use_lr_scheduler:\n",
    "            self.lr_scheduler = CosineWarmupScheduler(self.optimizer, warmup=warmup, max_iters=max_iters)\n",
    "        else:\n",
    "            self.lr_scheduler = None\n",
    "\n",
    "    def _calculate_loss(self, batch):\n",
    "        # get data and transform categories to one-hot vectors\n",
    "        inp_data, labels = batch\n",
    "        inp_data = F.one_hot(inp_data, num_classes=self.num_classes).float().to(device)\n",
    "        labels = labels.to(device)\n",
    "        # perform prediction and calculate loss and accuracy\n",
    "        preds = self.model(inp_data, add_positional_encoding=self.positional_encoding)\n",
    "        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        return loss, acc\n",
    "\n",
    "    def _step(self, batch):\n",
    "        loss, _ = self._calculate_loss(batch)\n",
    "        return loss\n",
    "\n",
    "    def train(self, train_loader, epoch_idx):\n",
    "        self.model.train()\n",
    "        train_loss = 0.\n",
    "        # get subsequent batches over the data in a given epoch\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self._step(batch)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step(batch_idx + epoch_idx*len(train_loader))\n",
    "\n",
    "            #print(self.lr_scheduler.get_last_lr())\n",
    "            train_loss += loss.item()\n",
    "        return train_loss/len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader, epoch_idx):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.\n",
    "        val_acc = 0.\n",
    "        # get subsequent batches over the data in a given epoch\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            loss, acc = self._calculate_loss(batch)\n",
    "            val_loss  += loss.item()\n",
    "            val_acc  += acc.item()\n",
    "        return val_loss/len(val_loader), val_acc/len(val_loader)\n",
    "\n",
    "    def plot_loss(self, train_losses, val_losses, title=None):\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"subsequent epochs\")\n",
    "        plt.ylabel('loss')\n",
    "        xlabels = np.linspace(0, model_args['epochs'], num=len(train_losses), endpoint=True)\n",
    "        plt.plot(xlabels, train_losses, label='training')\n",
    "        plt.plot(xlabels, val_losses, label='validation')\n",
    "        plt.legend()\n",
    "        if title is not None: plt.title(title)\n",
    "        plt.show()\n",
    "\n"
   ],
   "id": "6937b6797cf07e71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ],
   "id": "404852c2f65b7ee5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=model_args['batch_size'],\n",
    "                               shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(dataset(5000), batch_size=model_args['batch_size'])\n",
    "test_loader  = data.DataLoader(dataset(10000), batch_size=model_args['batch_size'])\n",
    "\n",
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ],
   "id": "6c3e179ba5a46acb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
    "    if input_data is not None:\n",
    "        input_data = input_data[idx].detach().cpu().numpy()\n",
    "    else:\n",
    "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
    "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
    "\n",
    "    num_heads = attn_maps[0].shape[0]\n",
    "    num_layers = len(attn_maps)\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):\n",
    "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            ax[row][column].set_yticklabels(input_data.tolist())\n",
    "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n",
    "data_input, labels = next(iter(val_loader))\n"
   ],
   "id": "8c1905dbf3d93163"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tasks to do\n",
    "* try to optimize the model hyperparameters (e.g. no. of layers, no. of heads within the layers), **it may be necessary to restart training several times**;\n",
    "* please play with different ingrediends of the Transformer model, e.g. turn-off positional encoding, or residual connections in the *EncoderBlock*, or use standard *lr_scheduler* (i.e. without warming-up);\n",
    "* please change the problem to learning a dataset with 0-9 digits in random order that we want to translate to sorted series (i.e. train the Transformer to learn sorting) -- please plot the attension map in that case -- do we have any understanding of this?"
   ],
   "id": "c466f5006310229f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_loader.dataset.num_categories",
   "id": "a44eaec4ebfcc7c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader))"
   ],
   "id": "c00effd2e9b5d13e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "          format(epoch_idx+1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "7901f7c69abf69a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "attention_maps = model.get_attention_maps(inp_data, True)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "bfd9f406dc2b0eaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. +++++++++++++++++++++++++++++++++++++++++++++++",
   "id": "af396a8844db804d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=False)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader))"
   ],
   "id": "416ab114edacacc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "          format(epoch_idx+1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "de94f1f2e2f8d869"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "attention_maps = model.get_attention_maps(inp_data, False)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "70d2cc64d4b93e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "not using learning rate scheduler",
   "id": "3f60eeb955244d63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "cd1474571e316db2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "84b8932c9b9843ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "attention_maps = model.get_attention_maps(inp_data, False)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "bd0a085f48cfb636"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. +++++++++++++++++++++++++++++++++++++++++++++++",
   "id": "6bc894f18c6d020b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SortDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        sort, _ = torch.sort(inp_data)\n",
    "        return inp_data, sort"
   ],
   "id": "4f8bc0231fefcfb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = partial(SortDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=model_args['batch_size'],\n",
    "                               shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(dataset(5000), batch_size=model_args['batch_size'], shuffle=True, drop_last=True, pin_memory=True)\n",
    "test_loader  = data.DataLoader(dataset(10000), batch_size=model_args['batch_size'], shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ],
   "id": "3ccd8c8bd2143179"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=1,\n",
    "                         num_layers=1).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories, add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader))"
   ],
   "id": "89e439423f5851ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "          format(epoch_idx+1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "3ac409961d62e14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bigger model",
   "id": "ad2c57d50f4d57f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories, add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader))"
   ],
   "id": "d968c905d7aaeef6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "          format(epoch_idx+1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "94dcd7224ede030e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "1e61a57cc098b2ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "without learning rate sheduler",
   "id": "b0d08d0d22894ea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "74500cb46479bc78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "514eb1184f58248b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "6ac4c02065987d9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One more layer",
   "id": "4aef041f3a922238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=4).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "ce180845d54d9790"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "d015c81164e0dd2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "4885bffadf8ecd27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MORE LAYERS",
   "id": "5fe4315502cc3a94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=5).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "8627c464b7852fbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "cc6af8a63efda422"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "3f8e2db5adebf693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "More heads",
   "id": "b6633225610393ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=5,\n",
    "                         num_layers=5).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "21d2b7e5cc6085f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "7f7a124df35d82f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MORE HEADS",
   "id": "42b34e984567748c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=10,\n",
    "                         num_layers=5).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "495278d972ec1f06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "cad979ff0db908a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "5134249998a4d804"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cee9008ce4316d7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
