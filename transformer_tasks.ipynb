{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Second part of laboratories were conducted by <a href=\"https://github.com/jarek-pawlowski\">Jarosław Pawłowski</a>. He created notebooks we used in laboratories. I am really glad that I had him as a tutor.\n",
    "\n",
    "In second course of ML/AI we were build ML models using pytorch.\n",
    "\n",
    "In this notebook I will present only some task we had as a homework from notebook."
   ],
   "id": "bf4644bb1b620487"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.625117Z",
     "start_time": "2025-09-05T16:58:57.419323Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model_args = {}\n",
    "model_args['batch_size'] = 128\n",
    "\n",
    "# learning rate is how fast it will descend\n",
    "model_args['lr'] = 1.e-3\n",
    "# the number of epochs is the number of times you go through the full dataset\n",
    "model_args['epochs'] = 40\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.629209Z",
     "start_time": "2025-09-05T16:58:58.626122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define scaled-dot product\n",
    "def scaled_dot_product(q, k, v):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / np.sqrt(d_k)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ],
   "id": "d86a73c467d71c38",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.635092Z",
     "start_time": "2025-09-05T16:58:58.631223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define multi-head attention block\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "            Assumptions we made:\n",
    "            d_k = d_v = head_dim\n",
    "            num_heads*head_dim = embed_dim\n",
    "            d_out = embed_dim\n",
    "            D = d_model = input_dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        qkv = self.qkv_proj(x) # [batch_size, seq_length, 3*embed_dim]\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)  # [batch_size, seq_length, num_heads, 3*head_dim]\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, seq_length, 3*head_dim]\n",
    "        q, k, v = qkv.chunk(3, dim=-1) # [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v)\n",
    "        values = values.permute(0, 2, 1, 3) # [batch_size, seq_length, num_heads, head_dim]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim) # heads concatenation\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ],
   "id": "3fbb262cb4af5454",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.639921Z",
     "start_time": "2025-09-05T16:58:58.635092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward):\n",
    "        \"\"\"\n",
    "            input_dim - dimensionality of the input\n",
    "            num_heads - number of heads to use in the attention block\n",
    "            dim_feedforward - dimensionality of the hidden layer in the MLP\n",
    "            For simplicity we assume input_dim = embed_dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.MLP_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = x + attn_out  # residual connection\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.MLP_net(x)\n",
    "        x = x + linear_out  # residual connection\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps"
   ],
   "id": "dd4d0459f9103d30",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.645103Z",
     "start_time": "2025-09-05T16:58:58.640927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "            d_model - input dim\n",
    "            max_len - maximum length of a sequence to expect\n",
    "            Encoding same as in the original paper\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ],
   "id": "f230d4aaa011d2d9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.649806Z",
     "start_time": "2025-09-05T16:58:58.646119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ],
   "id": "6c322eab80e73a3a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.658934Z",
     "start_time": "2025-09-05T16:58:58.650811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, num_layers):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim = model_dim - dimensionality to use inside the Transformer\n",
    "            num_heads - number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers - number of encoder blocks to use\n",
    "            lr - learning rate in the optimizer\n",
    "            warmup - number of warmup steps\n",
    "            max_iters - number of maximum iterations the model is trained for\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.input_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=self.num_layers,\n",
    "                                              input_dim=self.input_dim,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                              dim_feedforward=2*self.input_dim)\n",
    "\n",
    "    def forward(self, x, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "            x - input features of shape [Batch, SeqLen, input_dim]\n",
    "        \"\"\"\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x)\n",
    "        return attention_maps\n",
    "\n",
    "\n",
    "class TrainingUtils():\n",
    "\n",
    "    def __init__(self, device, model, num_classes, add_positional_encoding):\n",
    "        self.model = model.to(device)\n",
    "        self.num_classes = num_classes\n",
    "        self.positional_encoding = add_positional_encoding\n",
    "\n",
    "    def define_optimizer(self, lr, warmup, max_iters, use_lr_scheduler=True):\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        if use_lr_scheduler:\n",
    "            self.lr_scheduler = CosineWarmupScheduler(self.optimizer, warmup=warmup, max_iters=max_iters)\n",
    "        else:\n",
    "            self.lr_scheduler = None\n",
    "\n",
    "    def _calculate_loss(self, batch):\n",
    "        # get data and transform categories to one-hot vectors\n",
    "        inp_data, labels = batch\n",
    "        inp_data = F.one_hot(inp_data, num_classes=self.num_classes).float().to(device)\n",
    "        labels = labels.to(device)\n",
    "        # perform prediction and calculate loss and accuracy\n",
    "        preds = self.model(inp_data, add_positional_encoding=self.positional_encoding)\n",
    "        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        return loss, acc\n",
    "\n",
    "    def _step(self, batch):\n",
    "        loss, _ = self._calculate_loss(batch)\n",
    "        return loss\n",
    "\n",
    "    def train(self, train_loader, epoch_idx):\n",
    "        self.model.train()\n",
    "        train_loss = 0.\n",
    "        # get subsequent batches over the data in a given epoch\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self._step(batch)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step(batch_idx + epoch_idx*len(train_loader))\n",
    "\n",
    "            #print(self.lr_scheduler.get_last_lr())\n",
    "            train_loss += loss.item()\n",
    "        return train_loss/len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader, epoch_idx):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.\n",
    "        val_acc = 0.\n",
    "        # get subsequent batches over the data in a given epoch\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            loss, acc = self._calculate_loss(batch)\n",
    "            val_loss  += loss.item()\n",
    "            val_acc  += acc.item()\n",
    "        return val_loss/len(val_loader), val_acc/len(val_loader)\n",
    "\n",
    "    def plot_loss(self, train_losses, val_losses, title=None):\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"subsequent epochs\")\n",
    "        plt.ylabel('loss')\n",
    "        xlabels = np.linspace(0, model_args['epochs'], num=len(train_losses), endpoint=True)\n",
    "        plt.plot(xlabels, train_losses, label='training')\n",
    "        plt.plot(xlabels, val_losses, label='validation')\n",
    "        plt.legend()\n",
    "        if title is not None: plt.title(title)\n",
    "        plt.show()\n",
    "\n"
   ],
   "id": "6937b6797cf07e71",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.662841Z",
     "start_time": "2025-09-05T16:58:58.658934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ],
   "id": "404852c2f65b7ee5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.673902Z",
     "start_time": "2025-09-05T16:58:58.663847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=model_args['batch_size'],\n",
    "                               shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(dataset(5000), batch_size=model_args['batch_size'])\n",
    "test_loader  = data.DataLoader(dataset(10000), batch_size=model_args['batch_size'])\n",
    "\n",
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ],
   "id": "6c3e179ba5a46acb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([4, 0, 9, 4, 2, 1, 4, 6, 0, 1, 9, 9, 4, 2, 1, 6])\n",
      "Labels:     tensor([6, 1, 2, 4, 9, 9, 1, 0, 6, 4, 1, 2, 4, 9, 0, 4])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.680823Z",
     "start_time": "2025-09-05T16:58:58.674407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
    "    if input_data is not None:\n",
    "        input_data = input_data[idx].detach().cpu().numpy()\n",
    "    else:\n",
    "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
    "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
    "\n",
    "    num_heads = attn_maps[0].shape[0]\n",
    "    num_layers = len(attn_maps)\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):\n",
    "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            ax[row][column].set_yticklabels(input_data.tolist())\n",
    "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n",
    "data_input, labels = next(iter(val_loader))\n"
   ],
   "id": "8c1905dbf3d93163",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tasks to do\n",
    "* try to optimize the model hyperparameters (e.g. no. of layers, no. of heads within the layers), **it may be necessary to restart training several times**;\n",
    "* please play with different ingrediends of the Transformer model, e.g. turn-off positional encoding, or residual connections in the *EncoderBlock*, or use standard *lr_scheduler* (i.e. without warming-up);\n",
    "* please change the problem to learning a dataset with 0-9 digits in random order that we want to translate to sorted series (i.e. train the Transformer to learn sorting) -- please plot the attension map in that case -- do we have any understanding of this?"
   ],
   "id": "c466f5006310229f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:58.685618Z",
     "start_time": "2025-09-05T16:58:58.681963Z"
    }
   },
   "cell_type": "code",
   "source": "train_loader.dataset.num_categories",
   "id": "a44eaec4ebfcc7c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:58:59.456725Z",
     "start_time": "2025-09-05T16:58:58.687631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader))"
   ],
   "id": "c00effd2e9b5d13e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "PositionalEncoding-1               [-1, 16, 10]               0\n",
      "            Linear-2               [-1, 16, 30]             330\n",
      "            Linear-3               [-1, 16, 10]             110\n",
      "MultiheadAttention-4               [-1, 16, 10]               0\n",
      "         LayerNorm-5               [-1, 16, 10]              20\n",
      "            Linear-6               [-1, 16, 20]             220\n",
      "              ReLU-7               [-1, 16, 20]               0\n",
      "            Linear-8               [-1, 16, 10]             210\n",
      "         LayerNorm-9               [-1, 16, 10]              20\n",
      "     EncoderBlock-10               [-1, 16, 10]               0\n",
      "           Linear-11               [-1, 16, 30]             330\n",
      "           Linear-12               [-1, 16, 10]             110\n",
      "MultiheadAttention-13               [-1, 16, 10]               0\n",
      "        LayerNorm-14               [-1, 16, 10]              20\n",
      "           Linear-15               [-1, 16, 20]             220\n",
      "             ReLU-16               [-1, 16, 20]               0\n",
      "           Linear-17               [-1, 16, 10]             210\n",
      "        LayerNorm-18               [-1, 16, 10]              20\n",
      "     EncoderBlock-19               [-1, 16, 10]               0\n",
      "           Linear-20               [-1, 16, 30]             330\n",
      "           Linear-21               [-1, 16, 10]             110\n",
      "MultiheadAttention-22               [-1, 16, 10]               0\n",
      "        LayerNorm-23               [-1, 16, 10]              20\n",
      "           Linear-24               [-1, 16, 20]             220\n",
      "             ReLU-25               [-1, 16, 20]               0\n",
      "           Linear-26               [-1, 16, 10]             210\n",
      "        LayerNorm-27               [-1, 16, 10]              20\n",
      "     EncoderBlock-28               [-1, 16, 10]               0\n",
      "TransformerEncoder-29               [-1, 16, 10]               0\n",
      "================================================================\n",
      "Total params: 2,730\n",
      "Trainable params: 2,730\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T17:00:55.305007Z",
     "start_time": "2025-09-05T16:58:59.457729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "          format(epoch_idx+1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "7901f7c69abf69a6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\PycharmProjects\\ecg_class\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 | training loss = 2.5767 | validation loss = 2.4575 | validation acc 0.1102\n",
      "Train Epoch 2 | training loss = 2.2583 | validation loss = 1.7956 | validation acc 0.3968\n",
      "Train Epoch 3 | training loss = 1.1774 | validation loss = 0.8258 | validation acc 0.8004\n",
      "Train Epoch 4 | training loss = 0.6880 | validation loss = 0.5832 | validation acc 0.8598\n",
      "Train Epoch 5 | training loss = 0.5260 | validation loss = 0.4748 | validation acc 0.8816\n",
      "Train Epoch 6 | training loss = 0.4368 | validation loss = 0.4051 | validation acc 0.8904\n",
      "Train Epoch 7 | training loss = 0.3724 | validation loss = 0.3523 | validation acc 0.8993\n",
      "Train Epoch 8 | training loss = 0.3248 | validation loss = 0.3010 | validation acc 0.9113\n",
      "Train Epoch 9 | training loss = 0.2826 | validation loss = 0.2642 | validation acc 0.9198\n",
      "Train Epoch 10 | training loss = 0.2489 | validation loss = 0.2384 | validation acc 0.9267\n",
      "Train Epoch 11 | training loss = 0.2257 | validation loss = 0.2142 | validation acc 0.9318\n",
      "Train Epoch 12 | training loss = 0.2086 | validation loss = 0.2007 | validation acc 0.9341\n",
      "Train Epoch 13 | training loss = 0.1961 | validation loss = 0.2031 | validation acc 0.9335\n",
      "Train Epoch 14 | training loss = 0.1874 | validation loss = 0.1813 | validation acc 0.9384\n",
      "Train Epoch 15 | training loss = 0.1789 | validation loss = 0.1747 | validation acc 0.9406\n",
      "Train Epoch 16 | training loss = 0.1680 | validation loss = 0.1589 | validation acc 0.9461\n",
      "Train Epoch 17 | training loss = 0.1542 | validation loss = 0.1482 | validation acc 0.9486\n",
      "Train Epoch 18 | training loss = 0.1431 | validation loss = 0.1391 | validation acc 0.9517\n",
      "Train Epoch 19 | training loss = 0.1336 | validation loss = 0.1277 | validation acc 0.9567\n",
      "Train Epoch 20 | training loss = 0.1241 | validation loss = 0.1158 | validation acc 0.9616\n",
      "Train Epoch 21 | training loss = 0.1121 | validation loss = 0.1090 | validation acc 0.9642\n",
      "Train Epoch 22 | training loss = 0.1022 | validation loss = 0.0962 | validation acc 0.9661\n",
      "Train Epoch 23 | training loss = 0.0981 | validation loss = 0.0911 | validation acc 0.9677\n",
      "Train Epoch 24 | training loss = 0.0925 | validation loss = 0.0886 | validation acc 0.9687\n",
      "Train Epoch 25 | training loss = 0.0893 | validation loss = 0.0878 | validation acc 0.9687\n",
      "Train Epoch 26 | training loss = 0.0865 | validation loss = 0.0797 | validation acc 0.9720\n",
      "Train Epoch 27 | training loss = 0.0802 | validation loss = 0.0764 | validation acc 0.9727\n",
      "Train Epoch 28 | training loss = 0.0759 | validation loss = 0.0748 | validation acc 0.9730\n",
      "Train Epoch 29 | training loss = 0.0735 | validation loss = 0.0707 | validation acc 0.9750\n",
      "Train Epoch 30 | training loss = 0.0718 | validation loss = 0.0696 | validation acc 0.9746\n",
      "Train Epoch 31 | training loss = 0.0710 | validation loss = 0.0694 | validation acc 0.9751\n",
      "Train Epoch 32 | training loss = 0.0694 | validation loss = 0.0676 | validation acc 0.9754\n",
      "Train Epoch 33 | training loss = 0.0685 | validation loss = 0.0671 | validation acc 0.9756\n",
      "Train Epoch 34 | training loss = 0.0678 | validation loss = 0.0664 | validation acc 0.9759\n",
      "Train Epoch 35 | training loss = 0.0673 | validation loss = 0.0660 | validation acc 0.9761\n",
      "Train Epoch 36 | training loss = 0.0668 | validation loss = 0.0662 | validation acc 0.9759\n",
      "Train Epoch 37 | training loss = 0.0666 | validation loss = 0.0656 | validation acc 0.9761\n",
      "Train Epoch 38 | training loss = 0.0664 | validation loss = 0.0656 | validation acc 0.9761\n",
      "Train Epoch 39 | training loss = 0.0662 | validation loss = 0.0655 | validation acc 0.9761\n",
      "Train Epoch 40 | training loss = 0.0662 | validation loss = 0.0655 | validation acc 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T17:00:55.806473Z",
     "start_time": "2025-09-05T17:00:55.306023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attention_maps = model.get_attention_maps(inp_data, True)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "bfd9f406dc2b0eaf",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m attention_maps \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_attention_maps\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m plot_attention_maps(data_input, attention_maps, idx\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg_class\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[7], line 41\u001B[0m, in \u001B[0;36mTransformerModel.get_attention_maps\u001B[1;34m(self, x, add_positional_encoding)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;124;03mFunction for extracting the attention matrices of the whole Transformer for a single batch.\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;124;03mInput arguments same as the forward pass.\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m add_positional_encoding:\n\u001B[1;32m---> 41\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpositional_encoding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m attention_maps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer\u001B[38;5;241m.\u001B[39mget_attention_maps(x)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m attention_maps\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg_class\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ecg_class\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[5], line 21\u001B[0m, in \u001B[0;36mPositionalEncoding.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 21\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpe[:, :\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m]\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[1;31mIndexError\u001B[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. +++++++++++++++++++++++++++++++++++++++++++++++",
   "id": "af396a8844db804d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=False)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader))"
   ],
   "id": "416ab114edacacc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "          format(epoch_idx+1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "de94f1f2e2f8d869",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "attention_maps = model.get_attention_maps(inp_data, False)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "70d2cc64d4b93e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "not using learning rate scheduler",
   "id": "3f60eeb955244d63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "cd1474571e316db2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "84b8932c9b9843ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "attention_maps = model.get_attention_maps(inp_data, False)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "bd0a085f48cfb636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. +++++++++++++++++++++++++++++++++++++++++++++++",
   "id": "6bc894f18c6d020b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SortDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        sort, _ = torch.sort(inp_data)\n",
    "        return inp_data, sort"
   ],
   "id": "4f8bc0231fefcfb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = partial(SortDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=model_args['batch_size'],\n",
    "                               shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(dataset(5000), batch_size=model_args['batch_size'], shuffle=True, drop_last=True, pin_memory=True)\n",
    "test_loader  = data.DataLoader(dataset(10000), batch_size=model_args['batch_size'], shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ],
   "id": "3ccd8c8bd2143179",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=1,\n",
    "                         num_layers=1).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories, add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader))"
   ],
   "id": "89e439423f5851ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "          format(epoch_idx+1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "3ac409961d62e14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bigger model",
   "id": "ad2c57d50f4d57f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories, add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader))"
   ],
   "id": "d968c905d7aaeef6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "          format(epoch_idx+1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "94dcd7224ede030e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "1e61a57cc098b2ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "without learning rate sheduler",
   "id": "b0d08d0d22894ea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=3).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "74500cb46479bc78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "514eb1184f58248b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "6ac4c02065987d9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One more layer",
   "id": "4aef041f3a922238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=4).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "ce180845d54d9790",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "d015c81164e0dd2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "4885bffadf8ecd27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MORE LAYERS",
   "id": "5fe4315502cc3a94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=2,\n",
    "                         num_layers=5).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "8627c464b7852fbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "cc6af8a63efda422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "3f8e2db5adebf693",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "More heads",
   "id": "b6633225610393ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=5,\n",
    "                         num_layers=5).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "21d2b7e5cc6085f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "7f7a124df35d82f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MORE HEADS",
   "id": "42b34e984567748c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TransformerModel(input_dim=train_loader.dataset.num_categories,\n",
    "                         num_heads=10,\n",
    "                         num_layers=5).to(device)\n",
    "summary(model, input_size=(16,10))\n",
    "\n",
    "utils = TrainingUtils(device=device,\n",
    "                      model=model,\n",
    "                      num_classes=train_loader.dataset.num_categories,\n",
    "                      add_positional_encoding=True)\n",
    "utils.define_optimizer(lr=model_args['lr'],\n",
    "                       warmup=500,\n",
    "                       max_iters=model_args['epochs']*len(train_loader), use_lr_scheduler=False)"
   ],
   "id": "495278d972ec1f06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "for epoch_idx in range(model_args['epochs']):\n",
    "    train_loss = utils.train(train_loader, epoch_idx)\n",
    "    val_loss, val_acc = utils.validate(val_loader, epoch_idx)\n",
    "    print('Train Epoch {} | training loss = {:.4f} | validation loss = {:.4f} | validation acc {:.4f}'.\n",
    "    format(epoch_idx + 1, train_loss, val_loss, val_acc))\n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(val_loss)\n",
    "\n",
    "utils.plot_loss(train_losses, validation_losses)"
   ],
   "id": "cad979ff0db908a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=val_loader.dataset.num_categories).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = model.get_attention_maps(inp_data)\n",
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ],
   "id": "5134249998a4d804",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cee9008ce4316d7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
