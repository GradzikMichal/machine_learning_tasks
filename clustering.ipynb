{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First part of laboratories were conducted by <a href=\"https://github.com/makskliczkowski\">Maksymilian Kliczkowski</a>. He created notebooks we used in laboratories and he help me and my friends with understanding basic and advanced topics in machine learning. I am really glad that I had him as a tutor.\n",
    "\n",
    "In first course of ML/AI we were build ML models from the ground using math.\n",
    "\n",
    "In this notebook I will present only some task we had as a homework from notebook about scikit-learn"
   ],
   "id": "f130cd8bdf948d5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data use throughout the notebook",
   "id": "53f22fc4ed9e90f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "your_idx            = 420\n",
    "\n",
    "# use that data throught the notebook\n",
    "X, y = make_classification(\n",
    "    n_samples       =   1000,           # observations\n",
    "    n_features      =   10,             # total features\n",
    "    n_informative   =   8,              # 'useful' features\n",
    "    n_classes       =   5,              # target/label\n",
    "    random_state    =   your_idx        # put your unique results here\n",
    ")\n",
    "X2d, y2d = make_gaussian_quantiles(\n",
    "    cov             =   3.,\n",
    "    n_samples       =   1000,\n",
    "    n_features      =   2,\n",
    "    n_classes       =   2,\n",
    "    random_state    =   your_idx)\n",
    "\n",
    "plt.scatter(X2d[:, 0], X2d[:, 1], c = y2d)\n",
    "plt.title(\"Example of 2D data for classification with 3 classes\")\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.1, random_state = your_idx)\n",
    "X_train2d, X_test2d, Y_train2d, Y_test2d = train_test_split(X2d, y2d, test_size = 0.1, random_state = your_idx)"
   ],
   "id": "ad7eda5d6e36705b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## KNN",
   "id": "68ec982b628545da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Implement the following naive algorithm for k-NN. Algorithm for prediction involves [1pt]:\n",
    "1. Set the memory for initial distances vector to.\n",
    "2. Set the vector of frequencies of size $k$ to zeros\n",
    "3. For all samples i $\\leftarrow N$ do:\n",
    "    1. current_distance $(i) \\leftarrow d(X_i, \\rm point)$\n",
    "4. Sort distances up and take $k$-first elements.\n",
    "5. Go through those elements and set the frequency of classes occurance.\n",
    "6. Take the class with the highest occurance rate.\n",
    "\n",
    "- plot the outliers\n",
    "- check the accuracy\n",
    "- define the distance measure\n",
    "- test the algorithm on both datasets"
   ],
   "id": "c53c5d35f8a2478e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "measure      =  lambda x,y : np.sqrt(np.sum((x-y)**2))\n",
    "def classify(point,\n",
    "             Xtrain,\n",
    "             Ytrain,\n",
    "             k   :  int,\n",
    "             cat :  int,\n",
    "             measure):\n",
    "    dist = np.zeros((len(Xtrain),2))\n",
    "    for i, x in enumerate(Xtrain):\n",
    "      dist[i] = [measure(point,x), Ytrain[i]]\n",
    "    dist = dist[dist[:, 0].argsort()]\n",
    "    kn = dist[:k]\n",
    "    unique = np.unique(kn[:,1])\n",
    "    return unique[0]\n",
    "\n",
    "# predict samples\n",
    "\n",
    "predictions = []\n",
    "for x in X_train2d:\n",
    "    predictions.append(classify(x, X_train2d, Y_train2d, 3, 2, measure))\n",
    "\n",
    "predictions     = np.array(predictions)\n",
    "badPredictions  = predictions[predictions != Y_train2d]\n",
    "badSamples      = X_train2d[predictions != Y_train2d]\n",
    "\n",
    "plt.scatter(X_train2d[:, 0], X_train2d[:, 1], c = Y_train2d)\n",
    "plt.title(f\"Example of 2D data for classification with 3 classes.\\n Accuracy = {np.sum(predictions == Y_train2d) / len(predictions):.1e}\")\n",
    "plt.scatter(badSamples[:, 0], badSamples[:, 1], c = badPredictions, marker = 'x', s = 100, label = 'Bad predictions')\n",
    "plt.legend()"
   ],
   "id": "d92842f51b7cffb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## K-mean clustering\n",
    "\n",
    "#### Implement the following naive algorithm for k-means:\n",
    "\n",
    "1. Initialization: Choose the number of clusters that you want to identify in the dataset. Randomly initialize cluster centroids. Each centroid represents the center of a cluster.\n",
    "2. Assignment: For each data point in the dataset, calculate the squared Euclidean distance to each centroid. Assign the data point to the cluster whose centroid is the closest.\n",
    "3. Update: Recalculate the centroids of the clusters based on the mean of all data points assigned to each cluster.\n",
    "4. Repeat: Repeat the Assignment and Update steps iteratively until convergence, which occurs when the centroids no longer change significantly or a predefined number of iterations is reached.\n",
    "5. Output: After given number of iterations or insignificant change of clusters set the final results.\n",
    "\n",
    "- plot the outliers\n",
    "- check the accuracy\n",
    "- define the distance measure\n",
    "- test the algorithm on both datasets"
   ],
   "id": "921d90c51be2d651"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "measure         =   lambda x,y : np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "# predict data\n",
    "def kmeansPredict(clusters, distance, X):\n",
    "    dist = np.inf\n",
    "    dist_idx = None\n",
    "    for i, centroid in enumerate(clusters):\n",
    "      d = distance(X, centroid)\n",
    "      if d < dist:\n",
    "        dist = d\n",
    "        dist_idx = i\n",
    "    return dist_idx\n",
    "\n",
    "def kmeans(X,\n",
    "           k            : int,\n",
    "           distance,\n",
    "           iterations   = 1000,\n",
    "           seed         = 253880):\n",
    "    centroids = []\n",
    "    for _ in range(k):\n",
    "      centroids.append([np.random.random(2)])\n",
    "    print(centroids)\n",
    "    assign = []\n",
    "    for j, x in enumerate(X):\n",
    "      dist = np.inf\n",
    "      dist_idx = 0\n",
    "      for i, centroid in enumerate(centroids):\n",
    "        d = distance(x, centroid)\n",
    "        if d < dist:\n",
    "          dist = d\n",
    "          dist_idx = i\n",
    "      assign.append([dist_idx, x])\n",
    "    assign = np.array(assign)\n",
    "    for k_i in range(k):\n",
    "      centroids[k_i] = np.mean(assign[assign[:, 0]==k_i][:,1])\n",
    "    return centroids\n",
    "\n",
    "# train me!\n",
    "clusters = kmeans(X_train2d, k = 2, distance = measure, iterations = 50)\n",
    "\n",
    "# predict samples\n",
    "predictions = []\n",
    "for x in X_train2d:\n",
    "    predictions.append(kmeansPredict(clusters, measure, x))\n",
    "\n",
    "predictions     = np.array(predictions)\n",
    "badPredictions  = predictions[predictions != Y_train2d]\n",
    "badSamples      = X_train2d[predictions != Y_train2d]\n",
    "\n",
    "plt.scatter(X_train2d[:, 0], X_train2d[:, 1], c = Y_train2d)\n",
    "plt.title(f\"Example of 2D data for classification with 3 classes.\\n Accuracy = {np.sum(predictions == Y_train2d) / len(predictions):.1e}\")\n",
    "plt.scatter(badSamples[:, 0], badSamples[:, 1], c = badPredictions, marker = 'x', s = 100, label = 'Bad predictions')\n",
    "plt.legend()"
   ],
   "id": "2257607418a2a888"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DBSCAN Clustering\n",
    "\n",
    "#### Implement the following naive algorithm for DBSCAN. [1pt]:\n",
    "\n",
    "1. Choose a random data point and check its neighbors within `eps` range.\n",
    "2. Check whether the point is a core point, border point or an outlier.\n",
    "3. If it is a core point (has more points than `minPts`), apply the search for its neighbors.\n",
    "4. If it is a border, add it to the cluster but search for a starting point again #1.\n",
    "5. If it is an outlier, mark it as such and search for a starting point again #1.\n",
    "\n",
    "- plot the outliers\n",
    "- check the accuracy\n",
    "- define the distance measure\n",
    "- test the algorithm on both datasets"
   ],
   "id": "53e8633f988c6525"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ast import copy_location\n",
    "import random\n",
    "def check_point_character(  eps     :   float,\n",
    "                            minPts  :   int,\n",
    "                            X_train,\n",
    "                            idx     :   int\n",
    "                        ):\n",
    "    '''\n",
    "    This function shall first check the distance of a given point to each other point.\n",
    "    Then it decides how to classify it with one hot encoding.\n",
    "    '''\n",
    "    dist = []\n",
    "    for i, x in enumerate(X_train):\n",
    "      dist.append([int(i), measure(x, X_train[idx])])\n",
    "    dist = np.array(dist)\n",
    "    sorter = lambda x: (x[1])\n",
    "    dist = sorted(dist, key=sorter)\n",
    "    dist = np.array(dist)\n",
    "    point_idx = np.argwhere(dist[1:,1]<=eps) + 1\n",
    "    point_idx = point_idx.T[0]\n",
    "    point_idx = dist[point_idx.astype(int)].astype(int)\n",
    "    numbOfPoints = len(point_idx)\n",
    "    if numbOfPoints >= minPts:\n",
    "      return np.array([0,0,1]), point_idx.T[0]  #core\n",
    "    elif numbOfPoints > 0:\n",
    "      return np.array([0,1,0]), point_idx.T[0]  #border\n",
    "    else:\n",
    "      return np.array([1,0,0]), point_idx.T[0]  #outlier\n",
    "\n",
    "check_point_character(5e-1, 5, X_train2d, 0)\n",
    "\n",
    "def neighbors(point, x_train_class, minPts, neighbors_list, cluster):\n",
    "    if len(neighbors_list) < minPts:\n",
    "      return neighbors_list\n",
    "    for nn in neighbors_list:\n",
    "      if nn not in cluster:\n",
    "        cluster.add(nn)\n",
    "        newnn = neighbors(nn, x_train_class, minPts, x_train_class[nn][2], cluster)\n",
    "        for nw in newnn:\n",
    "          cluster.add(nw)\n",
    "    return cluster\n",
    "\n",
    "def cluster_me_please(eps : float, minPts : int, X_train):\n",
    "    # start with the first cluster, 0 will be considered as noise\n",
    "    x_train_class = []\n",
    "    for i, x in enumerate(X_train):\n",
    "      cat, nn = check_point_character(eps, minPts, X_train, i)\n",
    "      x_train_class.append(np.array([np.array([int(i)]), cat, nn],dtype=object))\n",
    "    corePoint = []\n",
    "    borderPoint = []\n",
    "    outlierPoint = []\n",
    "    for pC in x_train_class:\n",
    "      if (pC[1]==np.array([0,0,1])).all():\n",
    "        corePoint.append(pC)\n",
    "      elif (pC[1]==np.array([0,1,0])).all():\n",
    "        borderPoint.append(pC)\n",
    "      else:\n",
    "        outlierPoint.append(pC)\n",
    "    CPY = []\n",
    "    idxx = 0\n",
    "    points_in_cluster = 0\n",
    "    all_cpy = []\n",
    "    while points_in_cluster < len(X_train)- len(outlierPoint):\n",
    "\n",
    "        points_in_cluster = 0\n",
    "        cluster = set()\n",
    "        cluster = list((neighbors(corePoint[idxx][0], x_train_class, minPts,corePoint[idxx][2], cluster)))\n",
    "        idxx = -1\n",
    "        CPY.append(cluster)\n",
    "        all_cpy = all_cpy + cluster\n",
    "        in_cpy = True\n",
    "        for i in range(len(X_train)):\n",
    "            if i not in all_cpy:\n",
    "              if (x_train_class[i][1]==np.array([0,0,1])).all():\n",
    "                idxx=i\n",
    "                break\n",
    "        for i, cP in enumerate(corePoint):\n",
    "          if cP[0] == idxx:\n",
    "            idxx = i\n",
    "        for i in range(len(CPY)):\n",
    "          points_in_cluster += len(CPY[i])\n",
    "        if idxx == -1:\n",
    "          break\n",
    "\n",
    "    Y = np.ones(len(X_train)) * (len(CPY)+1)\n",
    "    for i, CPY_i in enumerate(CPY):\n",
    "      for cp in CPY_i:\n",
    "        Y[cp] = i\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions     = cluster_me_please(5e-1, 2, X_train2d)\n",
    "badPredictions  = predictions[predictions != Y_train2d]\n",
    "badSamples      = X_train2d[predictions != Y_train2d]\n",
    "\n",
    "plt.scatter(X_train2d[:, 0], X_train2d[:, 1], c = Y_train2d)\n",
    "plt.title(f\"Example of 2D data for classification with 3 classes.\\n Accuracy = {np.sum(predictions == Y_train2d) / len(predictions):.1e}\")\n",
    "plt.scatter(badSamples[:, 0], badSamples[:, 1], c = badPredictions, marker = 'x', s = 100, label = 'Bad predictions')\n",
    "plt.legend()\n"
   ],
   "id": "200b547043f003a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implement PCA from scratch",
   "id": "f62acefa83247d68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x       =   np.linspace(0, 100, num = 1000)\n",
    "data    =   x.copy()\n",
    "for i,ii in enumerate(x):\n",
    "    sig =   ii if i < len(x) // 2 else x[-1] - ii\n",
    "    data[i] = x[i] + np.random.normal(0.0, sig)\n",
    "\n",
    "plt.scatter(x, data, s = 10)\n",
    "plt.show()\n",
    "x_mean = np.mean(x)\n",
    "x_STD = np.std(x)\n",
    "data_mean = np.mean(data)\n",
    "data_STD = np.std(data)\n",
    "stand_X = (x-x_mean)/x_STD\n",
    "stand_DATA = (data - data_mean)/data_STD\n",
    "COV_MATRIX = np.zeros((2,2))\n",
    "COV_MATRIX[0][0] = sum(stand_X*stand_X)/len(stand_X)\n",
    "COV_MATRIX[0][1] = sum(stand_X*stand_DATA)/len(stand_X)\n",
    "COV_MATRIX[1][0] = sum(stand_DATA*stand_X)/len(stand_X)\n",
    "COV_MATRIX[1][1] = sum(stand_DATA*stand_DATA)/len(stand_X)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(COV_MATRIX)\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:,idx]\n",
    "explained_var = np.cumsum(eigenvalues)/np.sum(eigenvalues)\n",
    "n_components = np.argmax(explained_var >= 0.90) + 1\n",
    "n_components\n",
    "u = eigenvectors[:,:n_components]\n",
    "Z = np.stack((stand_X, stand_DATA), axis=1)\n",
    "Z_pca = Z.dot(u)\n",
    "Z_pca\n",
    "plt.scatter(Z_pca[:,0],Z_pca[:,1])\n",
    "plt.title(\"'My' calculations\")\n",
    "plt.show()\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(Z)\n",
    "x_pca = pca.transform(Z)\n",
    "print(x_pca)"
   ],
   "id": "5c7e472e1efcfbfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d89b63125071107c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
