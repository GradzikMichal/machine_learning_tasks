{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Image Classification using advanced CNNs\n",
    "- we are going to use CIFAR-10 [dataset](https://www.cs.toronto.edu/~kriz/cifar.html) of 32x32 color images"
   ],
   "id": "32a9bcacf05cb270"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "model_args = {}\n",
    "# we will use batch size of 64 in Stochastic Gradient Descent (SGD) optimization of the network\n",
    "model_args['batch_size'] = 64\n",
    "# learning rate is how fast it will descend\n",
    "model_args['lr'] = .05\n",
    "# SGD momentum (default: .5) momentum is a moving average of gradients (it helps to keep direction)\n",
    "model_args['momentum'] = .5\n",
    "# the number of epochs is the number of times you go through the full dataset\n",
    "model_args['weight_decay'] = 5.e-4\n",
    "model_args['epochs'] = 30\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Download CIFAR-10 dataset and define models\n",
    "Similar as with MNIST we use torchvision to download the data"
   ],
   "id": "338ad5976a6ad222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!rm -r ./data\n",
    "# normalize dataset\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "#mean = [.5, .5, .5]\n",
    "#std = [.5, .5, .5]\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean, std)])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "tensored_cifar10 = []\n",
    "\n",
    "for pic in cifar10_train:\n",
    "     picture = pic[0].to(\"cuda:0\")\n",
    "     true_val = torch.tensor(pic[1], device= \"cuda:0\")\n",
    "     pic_tensored = (picture, true_val)\n",
    "     tensored_cifar10.append(pic_tensored)\n",
    "\n",
    "torch.save(tensored_cifar10, 'tensored_cifar10.pth')\n",
    "\n",
    "# we divide this data into training and validation subsets\n",
    "train_subset, validation_subset = torch.utils.data.random_split(tensored_cifar10, [40000, 10000])\n",
    "torch.save(train_subset, 'train_subset.pt')\n",
    "torch.save(validation_subset, 'validation_subset.pt')\n",
    "\n",
    "\n",
    "train_subset = torch.load('train_subset.pt', map_location=torch.device('cuda:0'))\n",
    "\n",
    "validation_subset = torch.load('validation_subset.pt', map_location=torch.device('cuda:0'))\n",
    "test_subset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_cifar10 = []\n",
    "for pic in test_subset:\n",
    "     picture = pic[0].to(\"cuda:0\")\n",
    "     true_val = torch.tensor(pic[1], device= \"cuda:0\")\n",
    "     pic_tensored = (picture, true_val)\n",
    "     test_cifar10.append(pic_tensored)\n",
    "torch.save(test_cifar10, 'test_cifar10.pth')\n",
    "\n",
    "test_cifar10 = torch.load('test_cifar10.pth', map_location=torch.device('cuda:0'))\n",
    "# subsample to speedup training (colab has notebook lifetime limit)\n",
    "train_subset = torch.utils.data.Subset(train_subset, range(20000))\n",
    "validation_subset = torch.utils.data.Subset(validation_subset, range(5000))\n",
    "test_cifar10 = torch.utils.data.Subset(test_cifar10, range(5000))\n"
   ],
   "id": "2f2171ac00d66335"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_subset = torch.load('train_subset.pt', map_location=torch.device('cuda:0'))\n",
    "\n",
    "validation_subset = torch.load('validation_subset.pt', map_location=torch.device('cuda:0'))\n",
    "test_cifar10 = torch.load('test_cifar10.pth', map_location=torch.device('cuda:0'))\n",
    "# subsample to speedup training (colab has notebook lifetime limit)\n",
    "train_subset = torch.utils.data.Subset(train_subset, range(20000))\n",
    "validation_subset = torch.utils.data.Subset(validation_subset, range(5000))\n",
    "test_subset = torch.utils.data.Subset(test_cifar10, range(5000))"
   ],
   "id": "4b9b9a32c4c2ca17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define dataloaders\n",
    "print(type(train_subset))\n",
    "# train_subset = torch.utils.data.TensorDataset(train_subset)\n",
    "\n",
    "train_loader = []\n",
    "train_data = []\n",
    "rowx = []\n",
    "rowy = []\n",
    "i = 0\n",
    "for (x, y) in train_subset:\n",
    "    if i % model_args['batch_size'] == model_args['batch_size']-1:\n",
    "        train_data.append([torch.stack(rowx, dim =0).to(\"cuda:0\"), torch.tensor(rowy, device=\"cuda:0\")])\n",
    "        rowx = []\n",
    "        rowy = []\n",
    "    else:\n",
    "        rowx.append(x)\n",
    "        rowy.append(y)\n",
    "    i += 1\n",
    "print(1)\n",
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validation_loader = []\n",
    "rowx = []\n",
    "rowy = []\n",
    "i = 0\n",
    "for x, y in validation_subset:\n",
    "    if i % model_args['batch_size'] == model_args['batch_size'] - 1:\n",
    "        validation_loader.append([torch.stack(rowx, dim =0).to(\"cuda:0\"), torch.tensor(rowy, device=\"cuda:0\")])\n",
    "        rowx = []\n",
    "        rowy = []\n",
    "    else:\n",
    "        rowx.append(x)\n",
    "        rowy.append(y)\n",
    "    i += 1\n",
    "print(1)\n",
    "print(len(validation_loader))\n",
    "print(validation_loader[0])\n",
    "\n",
    "\n",
    "\n",
    "test_loader = []\n",
    "rowx = []\n",
    "rowy = []\n",
    "i = 0\n",
    "for x, y in test_subset:\n",
    "    if i % model_args['batch_size'] == model_args['batch_size'] - 1:\n",
    "        \n",
    "        test_loader.append([torch.stack(rowx, dim =0).to(\"cuda:0\"), torch.tensor(rowy, device=\"cuda:0\")])\n",
    "        rowx = []\n",
    "        rowy = []\n",
    "    else:\n",
    "        rowx.append(x)\n",
    "        rowy.append(y)\n",
    "    i += 1\n",
    "print(1)\n",
    "print(len(test_loader))\n",
    "print(test_loader[0])\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "id": "ab4a4c77ff881d0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    '''\n",
    "    simple CNN model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model\n",
    "    '''\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "vgg_cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    'vgg22': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 256, 'M', 512, 512, 512, 512, 512, 'M', 512, 512, 512, 512, 512, 'M']\n",
    "}"
   ],
   "id": "1aeb4d371064a8ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(model, device, train_data, optimizer, criterion, epoch_number):\n",
    "    model.train()\n",
    "    train_loss = torch.tensor(0., device=device)\n",
    "    # get subsequent batches over the data in a given epoch\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        # send data tensors to GPU (or CPU)\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # this will zero out the gradients for this batch\n",
    "        optimizer.zero_grad()\n",
    "        # this will execute the forward() function\n",
    "        output = model(data)\n",
    "        # calculate loss using c\n",
    "        loss = criterion(output, target)\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "        # update the model weights (with assumed learning rate)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print('Train Epoch: {}'.format(epoch_number))\n",
    "    train_loss_len = torch.tensor(len(train_data), device=device)\n",
    "    train_loss /= train_loss_len\n",
    "    print('\\tTrain set: Average loss: {:.4f}'.format(train_loss))\n",
    "    return train_loss\n",
    "\n",
    "def test(model, device, test_data, criterion, message=None):\n",
    "    model.eval()\n",
    "    test_loss = torch.tensor(0., device=device)\n",
    "    correct = torch.tensor(0., device=device)\n",
    "    # this is just inference, we don't need to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_data:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # calculate and sum up batch loss\n",
    "            test_loss += criterion(output, target).cuda()\n",
    "            # get the index of class with the max probability\n",
    "            prediction = output.argmax(dim=1)\n",
    "            #_, predicted = torch.max(outputs.data, axis=1)\n",
    "            # item() returns value of the given tensor\n",
    "            correct += prediction.eq(target).sum().item()\n",
    "    test_loss /= len(test_data)\n",
    "    accuracy = correct / len(test_data)\n",
    "    if message is not None:\n",
    "        print('\\t{}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            message, test_loss, correct, len(test_data)*model_args['batch_size'], 100.*accuracy/model_args['batch_size']))\n",
    "    return test_loss.cpu(), accuracy.cpu()\n",
    "\n",
    "def run_training(model, criterion, optimizer, no_epochs, device):\n",
    "    train_loss = torch.tensor([0.], device=device)\n",
    "    validation_loss = torch.tensor([0.], device=device)\n",
    "    validation_accuracy = torch.tensor([0.], device=device)\n",
    "    test_accuracy = torch.tensor([0.], device=device)\n",
    "    for epoch_number in range(1, no_epochs+1):\n",
    "        train_loss = torch.cat((train_loss, torch.tensor([train(model, device, train_data, optimizer, criterion, epoch_number)], device=device)))\n",
    "        \n",
    "        val_loss, val_acc = test(model, device, validation_loader,\n",
    "                                criterion, 'Validation set')\n",
    "        \n",
    "        validation_loss = torch.cat((validation_loss, torch.tensor([val_loss], device=device)))\n",
    "\n",
    "        validation_accuracy = torch.cat((validation_accuracy, torch.tensor([val_acc], device=device)))\n",
    "        \n",
    "        # we also collect test accuracies for every epoch\n",
    "        _, test_acc = test(model, device, test_loader, criterion)\n",
    "        test_accuracy = torch.cat((test_accuracy, torch.tensor([test_acc], device=device)))\n",
    "\n",
    "    # and select test accuracy for the best epoch (with the highest validation accuracy)\n",
    "    train_loss = train_loss[1:]\n",
    "    validation_loss = validation_loss[1:]\n",
    "    validation_accuracy = validation_accuracy[1:]\n",
    "    test_accuracy = test_accuracy[1:]\n",
    "    index = torch.argmax(validation_accuracy)\n",
    "    \n",
    "    best_accuracy = test_accuracy[index]\n",
    "    return train_loss, validation_loss, best_accuracy\n",
    "\n",
    "def plot_loss(train_loss, validation_loss, title):\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"subsequent epochs\")\n",
    "    plt.ylabel('average loss')\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, 'o-', label='training')\n",
    "    plt.plot(range(1, len(validation_loss)+1), validation_loss, 'o-', label='validation')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ],
   "id": "c609fc96babf7472"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tasks to do:\n",
    "\n",
    "1. Check turning off the data normalization -- how this impacts network training.\n",
    "2. What happens if we put *weight_decay* = 0. and increase *momentum* to .9 for VGG11 model -- why is that? (hint: observe interplay between *learning rate* and *momentum*)\n",
    "3. Try to explain why the deeper VGG16 network trains longer than VGG11.\n",
    "4. Compare with performance for deeper VGGs: 19 or 22 -- do we observe saturation in accuracy or even *degradation* problem?\n",
    "5. Does ResNet18 (with similar depth) perform better, what for ResNet34? What about training time in this case?\n",
    "6. Does transfer learning speed up training?"
   ],
   "id": "d504f95d931c5357"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. Changing data normalization | weight decay",
   "id": "c6c964a46843a926"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=model_args['lr'],\n",
    "                      momentum=model_args['momentum'],\n",
    "                      weight_decay=0.)\n",
    "no_epochs = model_args['epochs']\n",
    "\n",
    "train_loss, val_loss, best_accuracy = run_training(model, criterion, optimizer, no_epochs)\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy))\n",
    "plot_loss(train_loss, val_loss, 'SimpleCNN model')"
   ],
   "id": "c26347439779b3dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Without normalization result is worse by 4%.",
   "id": "3a7f5f235e97fc9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. VGG test",
   "id": "2f181bb987d8477f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VGG(make_layers(vgg_cfg['vgg11'])).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=model_args['lr'],\n",
    "                      momentum=.9,\n",
    "                      weight_decay=0.)\n",
    "no_epochs = model_args['epochs']\n",
    "\n",
    "train_loss_11, val_loss_11, best_accuracy = run_training(model, criterion, optimizer, no_epochs)\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy))\n",
    "plot_loss(train_loss_11, val_loss_11, 'VGG11 model')"
   ],
   "id": "7c4d8d9847d0f06f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Divergence of loss instead of convergence because of high learning rate.",
   "id": "66ff10d7164a9616"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. The deeper model usually needs more time to train because of more weights need tuning and loss value is lost going deeper through model",
   "id": "fe77dcd7649e3b3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. VGG19",
   "id": "fb97f1aabf7af9db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VGG(make_layers(vgg_cfg['vgg19'])).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=model_args['lr'],\n",
    "                      momentum=model_args['momentum'],\n",
    "                      weight_decay=1.e-3)\n",
    "\n",
    "no_epochs = model_args['epochs']*2\n",
    "\n",
    "train_loss_19, val_loss_19, best_accuracy = run_training(model, criterion, optimizer, no_epochs)\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy))\n",
    "plot_loss(train_loss_19, val_loss_19, 'VGG19 model')"
   ],
   "id": "563fcfc7092fa151"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = time.time()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VGG(make_layers(vgg_cfg['vgg22'])).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=model_args['lr'] * 0.1,\n",
    "                      momentum=model_args['momentum'],\n",
    "                      weight_decay=1.e-3)\n",
    "\n",
    "no_epochs = torch.tensor(model_args['epochs']*2, device=device)\n",
    "\n",
    "train_loss_22, val_loss_22, best_accuracy = run_training(model, criterion, optimizer, no_epochs, device)\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy/model_args['batch_size']))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time needed: {:.0f}s\".format(end - start))\n",
    "plot_loss(train_loss_22.cpu(), val_loss_22.cpu(), 'VGG22 model')"
   ],
   "id": "9215b81d29ded35e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5. ResNet",
   "id": "411ddc9c91e605a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = time.time()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet34().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=model_args['lr'],\n",
    "                      momentum=model_args['momentum'],\n",
    "                      weight_decay=1.e-3)\n",
    "\n",
    "no_epochs = torch.tensor(model_args['epochs'], device=device)\n",
    "\n",
    "train_loss_resnet34, validation_loss_resnet34, best_accuracy_resnet34 = run_training(model, criterion, optimizer, no_epochs, device)\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy_resnet34/model_args['batch_size']))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time needed: {:.0f}s\".format(end - start))\n",
    "plot_loss(train_loss_resnet34.cpu(), validation_loss_resnet34.cpu(), 'ResNet34 model')"
   ],
   "id": "78258533400bca32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ResNet with changes in its structure",
   "id": "13d791c187ea871b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = time.time()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "cudnn.benchmark = True\n",
    "model = resnet.resnet34(pretrained=True)\n",
    "print(model)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "model.bn1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "model.relu = nn.Identity()  # only input layers\n",
    "model.maxpool = nn.Identity()  # only in input layers\n",
    "model.fc = nn.Linear(512, 10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=model_args['lr'],\n",
    "                      momentum=model_args['momentum'],\n",
    "                      weight_decay=1.e-3)\n",
    "\n",
    "no_epochs = torch.tensor(model_args['epochs'], device=device)\n",
    "\n",
    "train_loss_resnet43_pre, validation_loss_resnet43_pre, best_accuracy_resnet43_pre = run_training(model, criterion, optimizer, no_epochs, device)\n",
    "\n",
    "print('\\nTest accuracy for best epoch: {:.0f}%\\n'.format(100.*best_accuracy_resnet43_pre/model_args['batch_size']))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time needed: {:.0f}s\".format(end - start))\n",
    "plot_loss(train_loss_resnet43_pre.cpu(), validation_loss_resnet43_pre.cpu(), 'ResNet18 pretrained model')"
   ],
   "id": "2ffcb09ecdc2e947"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
